import { createContext, useContext, useState, useEffect, ReactNode, useRef } from 'react';
import { useLocation } from 'wouter';
import { useAuth } from '@/hooks/useAuth';

// Utility function to check mobile device compatibility
const checkMobileCompatibility = () => {
  const isMobile = /Android|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
  const isHTTPS = location.protocol === 'https:';
  const hasMediaDevices = navigator.mediaDevices && navigator.mediaDevices.getUserMedia;
  
  console.log('[CallContext] Mobile compatibility check:', {
    isMobile,
    isHTTPS,
    hasMediaDevices,
    userAgent: navigator.userAgent
  });
  
  return { isMobile, isHTTPS, hasMediaDevices };
};

// Function to get mobile-specific error message
const getMobileErrorMessage = (error: any) => {
  const { isMobile, isHTTPS, hasMediaDevices } = checkMobileCompatibility();
  
  let message = 'Gagal memulai panggilan. ';
  
  if (!hasMediaDevices) {
    message += 'Browser Anda tidak mendukung fitur media. Gunakan Chrome, Firefox, atau Safari terbaru.';
    return message;
  }
  
  if (isMobile && !isHTTPS) {
    message += 'Untuk keamanan, gunakan HTTPS (https://) di mobile device.';
    return message;
  }
  
  switch (error.name) {
    case 'NotAllowedError':
      if (isMobile) {
        message += 'Di mobile: Buka Settings browser â†’ Site permissions â†’ Camera/Microphone â†’ Izinkan untuk situs ini. Lalu refresh halaman.';
      } else {
        message += 'Klik ikon kamera/microphone di address bar dan pilih "Allow".';
      }
      break;
    case 'NotFoundError':
      message += isMobile 
        ? 'Pastikan microphone dan camera tidak sedang digunakan aplikasi lain di HP Anda.'
        : 'Periksa apakah microphone dan camera terhubung dengan benar.';
      break;
    case 'NotSupportedError':
      message += isMobile
        ? 'Gunakan Chrome atau Firefox terbaru di mobile device Anda.'
        : 'Browser tidak mendukung WebRTC. Gunakan Chrome atau Firefox.';
      break;
    case 'NotReadableError':
      message += 'Microphone/camera sedang digunakan aplikasi lain. Tutup aplikasi tersebut dan coba lagi.';
      break;
    default:
      message += isMobile
        ? 'Pastikan browser mendukung WebRTC dan permission media diizinkan.'
        : 'Periksa koneksi microphone dan camera.';
  }
  
  return message;
};

interface CallState {
  callId?: string;
  callType: 'audio' | 'video';
  status: 'idle' | 'calling' | 'ringing' | 'connected' | 'ended';
  isIncoming: boolean;
  peerUserId?: number;
  peerName?: string;
  localStream?: MediaStream;
  remoteStreams: Map<number, MediaStream>;
  peerConnection?: RTCPeerConnection;
  audioEnabled: boolean;
  videoEnabled: boolean;
  isMuted: boolean;
  startTime?: Date;
  // Group call specific fields
  isGroupCall?: boolean;
  groupId?: number;
  groupName?: string;
  participants?: Array<{
    userId: number;
    userName: string;
    audioEnabled: boolean;
    videoEnabled: boolean;
    stream: MediaStream | null;
  }>;
}

interface CallContextType {
  activeCall: CallState | null;
  incomingCall: CallState | null;
  remoteAudioStream: MediaStream | null;
  ws: WebSocket | null;
  startCall: (peerUserId: number, peerName: string, callType: 'audio' | 'video') => void;
  startGroupCall: (groupId: number, groupName: string, callType: 'audio' | 'video') => void;
  acceptCall: () => void;
  rejectCall: () => void;
  hangupCall: () => void;
  toggleCallAudio: () => void;
  toggleCallVideo: () => void;
  toggleMute: () => void;
  switchCallCamera: () => void;
}

const CallContext = createContext<CallContextType | undefined>(undefined);

export function CallProvider({ children }: { children: ReactNode }) {
  const { user } = useAuth();
  const [location, setLocation] = useLocation();
  const [activeCall, setActiveCall] = useState<CallState | null>(null);
  const [incomingCall, setIncomingCall] = useState<CallState | null>(null);
  const [ws, setWs] = useState<WebSocket | null>(null);
  const [ringtoneAudio, setRingtoneAudio] = useState<HTMLAudioElement | null>(null);
  const [waitingToneInterval, setWaitingToneInterval] = useState<NodeJS.Timeout | null>(null);
  const [audioContext, setAudioContext] = useState<AudioContext | null>(null);
  
  // Add ref for pending participant updates
  const pendingParticipantUpdatesRef = useRef<Array<{type: string, payload: any}>>([]);
  const [remoteAudioStream, setRemoteAudioStream] = useState<MediaStream | null>(null);
  
  // Audio notification function for new messages
  const playNewMessageSound = async () => {
    console.log('[CallContext] ðŸ”Š Attempting to play new message sound');
    
    try {
      // Method 1: Web Audio API (most reliable)
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      
      // Resume audio context if suspended
      if (audioContext.state === 'suspended') {
        await audioContext.resume();
      }
      
      // Create a simple beep sound
      const oscillator = audioContext.createOscillator();
      const gainNode = audioContext.createGain();
      
      oscillator.connect(gainNode);
      gainNode.connect(audioContext.destination);
      
      // High-low double beep pattern
      oscillator.frequency.setValueAtTime(800, audioContext.currentTime);
      oscillator.frequency.setValueAtTime(600, audioContext.currentTime + 0.1);
      
      gainNode.gain.setValueAtTime(0.3, audioContext.currentTime);
      gainNode.gain.exponentialRampToValueAtTime(0.01, audioContext.currentTime + 0.2);
      
      oscillator.start(audioContext.currentTime);
      oscillator.stop(audioContext.currentTime + 0.2);
      
      console.log('[CallContext] âœ… Audio notification played successfully');
      
    } catch (error) {
      console.log('[CallContext] âŒ Web Audio API failed, trying HTML5 Audio fallback');
      
      try {
        // Method 2: HTML5 Audio fallback
        const audio = new Audio('data:audio/wav;base64,UklGRnoGAABXQVZFZm10IBAAAAABAAEAQB8AAEAfAAABAAgAZGF0YQoGAACBhYqFbF1fdJivrJBhNjVgodDbq2EcBj+a2/LDciUFLIHO8tiJNwgZaLvt559NEAxQp+PwtmMcBjiR1/LMeSwFJHfH8N2QQAoUXrTp66hVFApGn+DyvmwhBjqY4PCBEDTUJJjXe');
        audio.volume = 0.3;
        await audio.play();
        
        console.log('[CallContext] âœ… HTML5 Audio notification played successfully');
        
      } catch (audioError) {
        console.log('[CallContext] âŒ HTML5 Audio failed, trying browser notification');
        
        // Method 3: Browser notification as fallback
        if (Notification.permission === 'granted') {
          new Notification('New Message', {
            body: 'You have a new message',
            icon: '/icon-192x192.png'
          });
        }
        
        // Method 4: Vibration API as last resort (mobile)
        if ('vibrate' in navigator) {
          navigator.vibrate([100, 50, 100]);
        }
      }
    }
  };

  // Restore group call state from localStorage when on group-call page
  useEffect(() => {
    if (location === '/group-call' && !activeCall && user) {
      const storedCall = localStorage.getItem('activeGroupCall');
      if (storedCall) {
        try {
          const callData = JSON.parse(storedCall);
          console.log('[CallContext] Restoring group call state from localStorage:', callData);
          
          // Recreate the call state for group call page
          const restoredCall: CallState = {
            callId: callData.callId,
            callType: callData.callType,
            status: 'connected',
            isIncoming: false,
            localStream: undefined,
            remoteStreams: new Map(),
            audioEnabled: true,
            videoEnabled: callData.callType === 'video',
            isMuted: false,
            startTime: new Date(),
            isGroupCall: true,
            groupId: callData.groupId,
            groupName: callData.groupName,
            participants: []
          };
          
          setActiveCall(restoredCall);
          console.log('[CallContext] Group call state restored successfully');
        } catch (error) {
          console.error('[CallContext] Error restoring group call state:', error);
          localStorage.removeItem('activeGroupCall');
        }
      }
    }
  }, [location, activeCall, user]);

  // Message handler function for WebSocket messages
  const handleWebSocketMessage = (message: any) => {
    switch (message.type) {
      case 'incoming_call':
        handleIncomingCall(message);
        break;
      case 'incoming_group_call':
        handleIncomingGroupCall(message);
        break;
      case 'call_accepted':
        handleCallAccepted(message);
        break;
      case 'call_ended':
        handleCallEnded(message);
        break;
      case 'group_call_ended':
        handleGroupCallEnded(message);
        break;
      case 'group_call_user_left':
        handleGroupCallUserLeft(message);
        break;
      case 'group_call_participants_update':
        handleGroupCallParticipantsUpdate(message);
        break;
      case 'initiate_group_webrtc':
        console.log('[CallContext] ðŸ”¥ ROUTING: initiate_group_webrtc case matched, calling handleInitiateGroupWebRTC');
        handleInitiateGroupWebRTC(message);
        break;
      case 'group_call_no_participants':
        handleGroupCallNoParticipants(message);
        break;
      case 'webrtc_ready':
        handleWebRTCReady(message);
        break;
      case 'webrtc_offer':
        handleWebRTCOffer(message);
        break;
      case 'webrtc_answer':
        handleWebRTCAnswer(message);
        break;
      case 'webrtc_ice_candidate':
        handleWebRTCIceCandidate(message);
        break;
      default:
        // Ignore other message types (chat, user_status, etc.)
        break;
    }
  };
  
  // Use refs to store stable call state that won't be lost during re-renders
  const activeCallRef = useRef<CallState | null>(null);
  const incomingCallRef = useRef<CallState | null>(null);
  
  // Queue for ICE candidates that arrive before remote description is set
  const pendingIceCandidates = useRef<any[]>([]);
  
  // Queue for WebRTC offers that arrive before incoming call is created
  const pendingOffers = useRef<any[]>([]);

  // ðŸŽ¯ CRITICAL FIX 9: Enhanced receiver-side RTP monitoring with auto-repair
  const startReceiverRTPMonitoring = (peerConnection: RTCPeerConnection, callId: string) => {
    console.log('[CallContext] ðŸ”„ RECEIVER: Starting enhanced receiver-side RTP monitoring for callId:', callId);
    
    let previousInboundStats = new Map<string, any>();
    let receiverStatsInterval: NodeJS.Timeout;
    let noRTPDataWarningCount = 0;
    let isReceivingRTP = false;
    let lastRTPReceiveTime = 0;
    
    const checkReceiverRTPFlow = async () => {
      if (!peerConnection || peerConnection.connectionState === 'closed') {
        return;
      }
      
      try {
        const stats = await peerConnection.getStats();
        let hasInboundAudio = false;
        let isReceivingAudioData = false;
        let receiverTrackIssues = [];
        
        stats.forEach((report) => {
          // ðŸŽ¯ CRITICAL: Monitor inbound RTP specifically for receiver validation
          if (report.type === 'inbound-rtp' && report.kind === 'audio') {
            hasInboundAudio = true;
            const previousReport = previousInboundStats.get(report.id);
            const bytesReceived = report.bytesReceived || 0;
            const packetsReceived = report.packetsReceived || 0;
            
            if (previousReport) {
              const bytesDelta = bytesReceived - (previousReport.bytesReceived || 0);
              const packetsDelta = packetsReceived - (previousReport.packetsReceived || 0);
              
              console.log('[CallContext] ðŸ“Š RECEIVER: Inbound RTP monitoring:', {
                callId,
                bytesReceived,
                bytesDelta,
                packetsReceived,
                packetsDelta,
                audioLevel: report.audioLevel,
                jitter: report.jitter,
                packetsLost: report.packetsLost,
                fractionLost: report.fractionLost,
                lastPacketReceivedTimestamp: report.lastPacketReceivedTimestamp
              });
              
              // ðŸŽ¯ CRITICAL: Detect actual RTP data flow
              if (bytesDelta > 0 && packetsDelta > 0) {
                isReceivingAudioData = true;
                lastRTPReceiveTime = Date.now();
                
                if (!isReceivingRTP) {
                  console.log('[CallContext] âœ… RECEIVER: RTP data flow detected - receiver working correctly!');
                  isReceivingRTP = true;
                  noRTPDataWarningCount = 0;
                }
              } else if (isReceivingRTP && bytesDelta === 0 && packetsDelta === 0) {
                // Check if RTP flow has stopped
                const timeSinceLastRTP = Date.now() - lastRTPReceiveTime;
                if (timeSinceLastRTP > 5000) { // 5 seconds without RTP
                  console.warn('[CallContext] âš ï¸ RECEIVER: RTP flow has stopped - may need recovery');
                  isReceivingRTP = false;
                }
              }
            }
            
            previousInboundStats.set(report.id, report);
          }
          
          // ðŸŽ¯ CRITICAL: Monitor receiver track state
          if (report.type === 'track' && report.kind === 'audio' && !report.remoteSource) {
            console.log('[CallContext] ðŸ“¡ RECEIVER: Local audio track state:', {
              trackIdentifier: report.trackIdentifier,
              ended: report.ended,
              detached: report.detached,
              frameWidth: report.frameWidth,
              frameHeight: report.frameHeight
            });
          }
        });
        
        // ðŸŽ¯ CRITICAL: Validate receiver state and implement auto-repair
        if (!hasInboundAudio) {
          receiverTrackIssues.push('No inbound RTP reports found');
        }
        
        if (!isReceivingAudioData && hasInboundAudio) {
          noRTPDataWarningCount++;
          receiverTrackIssues.push(`No RTP data flow (warning count: ${noRTPDataWarningCount})`);
          
          // ðŸŽ¯ CRITICAL: Auto-repair mechanism after multiple failed checks
          if (noRTPDataWarningCount >= 5) { // 10 seconds of no RTP data
            console.error('[CallContext] ðŸ”§ RECEIVER: CRITICAL AUTO-REPAIR - No RTP data for 10 seconds');
            
            // Attempt receiver track recovery
            await attemptReceiverTrackRecovery(peerConnection, callId);
            noRTPDataWarningCount = 0; // Reset counter after repair attempt
          }
        }
        
        // ðŸŽ¯ CRITICAL: Log receiver issues
        if (receiverTrackIssues.length > 0) {
          console.warn('[CallContext] âš ï¸ RECEIVER: Issues detected:', receiverTrackIssues);
          console.warn('[CallContext] ðŸ”§ RECEIVER: Transport state:', {
            connectionState: peerConnection.connectionState,
            iceConnectionState: peerConnection.iceConnectionState,
            signalingState: peerConnection.signalingState
          });
        }
        
      } catch (error) {
        console.error('[CallContext] âŒ RECEIVER: Error checking RTP flow:', error);
      }
    };
    
    // Check receiver RTP flow every 2 seconds
    receiverStatsInterval = setInterval(checkReceiverRTPFlow, 2000);
    
    // Initial check
    checkReceiverRTPFlow();
    
    // Store cleanup function
    (window as any)[`__receiverStatsCleanup_${callId}`] = () => {
      if (receiverStatsInterval) {
        clearInterval(receiverStatsInterval);
      }
    };
    
    return () => {
      if (receiverStatsInterval) {
        clearInterval(receiverStatsInterval);
      }
    };
  };
  
  // ðŸŽ¯ CRITICAL FIX 10: Receiver track recovery mechanism
  const attemptReceiverTrackRecovery = async (peerConnection: RTCPeerConnection, callId: string) => {
    console.log('[CallContext] ðŸ”§ RECEIVER: Attempting receiver track recovery for callId:', callId);
    
    try {
      const transceivers = peerConnection.getTransceivers();
      let audioReceiverTransceiver: RTCRtpTransceiver | null = null;
      
      // Find audio receiver transceiver
      transceivers.forEach((transceiver) => {
        if (transceiver.receiver.track?.kind === 'audio') {
          audioReceiverTransceiver = transceiver;
        }
      });
      
      if (audioReceiverTransceiver) {
        console.log('[CallContext] ðŸ”§ RECEIVER: Found audio receiver transceiver, attempting recovery');
        
        // Step 1: Force direction to recvonly temporarily
        const originalDirection = audioReceiverTransceiver.direction;
        audioReceiverTransceiver.direction = 'recvonly';
        
        // Step 2: Disable and re-enable receiver track
        const receiverTrack = audioReceiverTransceiver.receiver.track;
        if (receiverTrack) {
          console.log('[CallContext] ðŸ”§ RECEIVER: Cycling receiver track enable state');
          receiverTrack.enabled = false;
          
          setTimeout(() => {
            receiverTrack.enabled = true;
            console.log('[CallContext] ðŸ”§ RECEIVER: Receiver track re-enabled');
            
            // Step 3: Restore original direction
            setTimeout(() => {
              audioReceiverTransceiver.direction = originalDirection;
              console.log('[CallContext] ðŸ”§ RECEIVER: Direction restored to:', originalDirection);
            }, 100);
          }, 200);
        }
        
        // Step 4: Check if we need to restart ICE for connectivity
        const connectionState = peerConnection.connectionState;
        if (connectionState !== 'connected') {
          console.log('[CallContext] ðŸ”§ RECEIVER: Connection not stable, restarting ICE');
          peerConnection.restartIce();
        }
        
        console.log('[CallContext] âœ… RECEIVER: Recovery attempt completed');
        
      } else {
        console.error('[CallContext] âŒ RECEIVER: No audio receiver transceiver found for recovery');
      }
      
    } catch (error) {
      console.error('[CallContext] âŒ RECEIVER: Error during recovery attempt:', error);
    }
  };
  
  // ðŸŽ¯ CRITICAL FIX 11: Comprehensive transport state monitoring function
  const startTransportStateMonitoring = (peerConnection: RTCPeerConnection, callId: string) => {
    console.log('[CallContext] ðŸ”„ ENHANCED: Starting comprehensive transport state monitoring for callId:', callId);
    
    let previousStats = new Map<string, any>();
    let statsInterval: NodeJS.Timeout;
    let connectivityTimeout: NodeJS.Timeout;
    let isConnected = false;
    
    const checkConnectivity = async () => {
      if (!peerConnection || peerConnection.connectionState === 'closed') {
        return;
      }
      
      try {
        const stats = await peerConnection.getStats();
        let hasAudioFlow = false;
        let hasValidTransport = false;
        
        stats.forEach((report) => {
          // Check for actual audio data flow with delta calculations
          if (report.type === 'inbound-rtp' && report.kind === 'audio') {
            const previousReport = previousStats.get(report.id);
            const bytesReceived = report.bytesReceived || 0;
            const packetsReceived = report.packetsReceived || 0;
            
            if (previousReport) {
              const bytesDelta = bytesReceived - (previousReport.bytesReceived || 0);
              const packetsDelta = packetsReceived - (previousReport.packetsReceived || 0);
              
              console.log('[CallContext] ðŸ“Š ENHANCED: Audio RTP flow stats:', {
                callId,
                bytesReceived,
                bytesDelta,
                packetsReceived,
                packetsDelta,
                audioLevel: report.audioLevel,
                jitter: report.jitter,
                packetsLost: report.packetsLost
              });
              
              if (bytesDelta > 0 && packetsDelta > 0) {
                hasAudioFlow = true;
                if (!isConnected) {
                  console.log('[CallContext] âœ… ENHANCED: Audio RTP flow detected - call is truly connected!');
                  isConnected = true;
                }
              } else if (isConnected && bytesDelta === 0) {
                console.warn('[CallContext] âš ï¸ ENHANCED: Audio RTP flow stopped - potential connection issue');
              }
            }
            
            previousStats.set(report.id, report);
          }
          
          // Check outbound RTP stats for sender side validation
          if (report.type === 'outbound-rtp' && report.kind === 'audio') {
            const previousReport = previousStats.get(report.id);
            const bytesSent = report.bytesSent || 0;
            const packetsSent = report.packetsSent || 0;
            
            if (previousReport) {
              const bytesDelta = bytesSent - (previousReport.bytesSent || 0);
              const packetsDelta = packetsSent - (previousReport.packetsSent || 0);
              
              console.log('[CallContext] ðŸ“¤ ENHANCED: Audio sender stats:', {
                callId,
                bytesSent,
                bytesDelta,
                packetsSent,
                packetsDelta,
                retransmittedPacketsSent: report.retransmittedPacketsSent
              });
              
              if (bytesDelta === 0 && packetsDelta === 0) {
                console.warn('[CallContext] âš ï¸ ENHANCED: No outbound audio packets - sender track issue!');
              }
            }
            
            previousStats.set(report.id, report);
          }
          
          // Check transport connectivity
          if (report.type === 'transport') {
            console.log('[CallContext] ðŸŒ ENHANCED: Transport stats:', {
              callId,
              dtlsState: report.dtlsState,
              iceState: report.iceState,
              selectedCandidatePairId: report.selectedCandidatePairId
            });
            
            if (report.dtlsState === 'connected' && report.iceState === 'connected') {
              hasValidTransport = true;
            }
          }
          
          // Check candidate pair for actual connectivity
          if (report.type === 'candidate-pair' && report.state === 'succeeded') {
            console.log('[CallContext] ðŸ§ª ENHANCED: Active candidate pair:', {
              callId,
              state: report.state,
              priority: report.priority,
              bytesSent: report.bytesSent,
              bytesReceived: report.bytesReceived,
              localCandidateId: report.localCandidateId,
              remoteCandidateId: report.remoteCandidateId
            });
            hasValidTransport = true;
          }
        });
        
        // Alert if no audio flow detected after reasonable time
        if (!hasAudioFlow && isConnected === false) {
          const now = Date.now();
          const callStartTime = (window as any).__callStartTime || now;
          const timeSinceStart = now - callStartTime;
          
          if (timeSinceStart > 10000) { // 10 seconds
            console.error('[CallContext] âŒ CRITICAL: No audio RTP flow detected after 10 seconds!');
            console.error('[CallContext] ðŸ”§ Potential fixes needed:');
            console.error('[CallContext] 1. Check if local tracks were added before offer creation');
            console.error('[CallContext] 2. Verify transceiver directions are sendrecv');
            console.error('[CallContext] 3. Check for DTLS/ICE connectivity issues');
          }
        }
        
      } catch (error) {
        console.error('[CallContext] âŒ Error checking connectivity stats:', error);
      }
    };
    
    // Start monitoring every 2 seconds
    statsInterval = setInterval(checkConnectivity, 2000);
    
    // Initial check
    checkConnectivity();
    
    // Set up connection state change monitoring
    const onConnectionStateChange = () => {
      const state = peerConnection.connectionState;
      console.log('[CallContext] ðŸ”„ ENHANCED: Connection state changed to:', state);
      
      if (state === 'connected') {
        console.log('[CallContext] âœ… ENHANCED: WebRTC connection established');
        (window as any).__callStartTime = Date.now();
      } else if (state === 'failed') {
        console.error('[CallContext] âŒ ENHANCED: WebRTC connection failed');
        
        // Attempt ICE restart as recovery
        console.log('[CallContext] ðŸ”„ ENHANCED: Attempting ICE restart for recovery');
        peerConnection.restartIce();
      }
    };
    
    const onIceConnectionStateChange = () => {
      const state = peerConnection.iceConnectionState;
      console.log('[CallContext] ðŸ§Š ENHANCED: ICE connection state changed to:', state);
      
      if (state === 'connected' || state === 'completed') {
        console.log('[CallContext] âœ… ENHANCED: ICE connection established');
      } else if (state === 'failed') {
        console.error('[CallContext] âŒ ENHANCED: ICE connection failed');
      }
    };
    
    peerConnection.addEventListener('connectionstatechange', onConnectionStateChange);
    peerConnection.addEventListener('iceconnectionstatechange', onIceConnectionStateChange);
    
    // Store cleanup function globally for cleanup
    (window as any)[`__statsCleanup_${callId}`] = () => {
      if (statsInterval) {
        clearInterval(statsInterval);
      }
      if (connectivityTimeout) {
        clearTimeout(connectivityTimeout);
      }
      peerConnection.removeEventListener('connectionstatechange', onConnectionStateChange);
      peerConnection.removeEventListener('iceconnectionstatechange', onIceConnectionStateChange);
      console.log('[CallContext] ðŸ§¹ ENHANCED: Transport monitoring cleanup completed for callId:', callId);
    };
  };

  // Function to aggressively stop all ringtones (using waiting tone pattern)
  const stopAllRingtones = () => {
    console.log('[CallContext] ðŸ”‡ STOPPING ALL RINGTONES - comprehensive cleanup');
    
    // Stop HTML5 ringtone audio - same pattern as waiting tone
    if (ringtoneAudio) {
      try {
        console.log('[CallContext] Stopping HTML5 ringtone audio');
        ringtoneAudio.pause();
        ringtoneAudio.currentTime = 0;
        ringtoneAudio.volume = 0;
        ringtoneAudio.muted = true;
        ringtoneAudio.src = '';
        ringtoneAudio.srcObject = null;
        ringtoneAudio.load();
        setRingtoneAudio(null);
        console.log('[CallContext] âœ… HTML5 ringtone stopped and cleared');
      } catch (e) {
        console.log('[CallContext] Error stopping HTML5 ringtone:', e);
      }
    }
    
    // Stop current global ringtone reference (same pattern as oscillator)
    if ((window as any).__currentRingtone) {
      try {
        console.log('[CallContext] Force stopping current global ringtone');
        (window as any).__currentRingtone.pause();
        (window as any).__currentRingtone.currentTime = 0;
        (window as any).__currentRingtone.volume = 0;
        (window as any).__currentRingtone.muted = true;
        (window as any).__currentRingtone.src = '';
        (window as any).__currentRingtone.load();
        (window as any).__currentRingtone = null;
        console.log('[CallContext] âœ… Global ringtone reference stopped');
      } catch (e) {
        console.log('[CallContext] Global ringtone already stopped or error:', e);
      }
    }
    
    // Find and stop ALL audio elements playing ringtones
    try {
      const audioElements = document.querySelectorAll('audio');
      let stoppedCount = 0;
      audioElements.forEach((audio: HTMLAudioElement, index: number) => {
        // Stop any audio that's currently playing or has ringtone-related source
        if (!audio.paused || (audio.src && (audio.src.includes('ringtone') || audio.src.includes('ring')))) {
          console.log(`[CallContext] Force stopping audio element ${index}:`, audio.src || 'no src');
          audio.pause();
          audio.currentTime = 0;
          audio.volume = 0;
          audio.muted = true;
          audio.src = '';
          audio.srcObject = null;
          audio.load();
          stoppedCount++;
        }
      });
      console.log(`[CallContext] âœ… Stopped ${stoppedCount} audio elements`);
    } catch (e) {
      console.log('[CallContext] Error stopping audio elements:', e);
    }
    
    // Stop Web Audio API ringtone gain node (same pattern as waiting tone)
    if ((window as any).__currentRingtoneGainNode) {
      try {
        console.log('[CallContext] Force disconnecting current ringtone gain node');
        (window as any).__currentRingtoneGainNode.disconnect();
        (window as any).__currentRingtoneGainNode = null;
        console.log('[CallContext] âœ… Ringtone gain node disconnected');
      } catch (e) {
        console.log('[CallContext] Ringtone gain node already disconnected or error:', e);
      }
    }
    
    // Stop Web Audio API ringtone source (same pattern as waiting tone)
    if ((window as any).__ringtoneSource) {
      try {
        console.log('[CallContext] Force stopping ringtone source');
        (window as any).__ringtoneSource.stop();
        (window as any).__ringtoneSource.disconnect();
        (window as any).__ringtoneSource = null;
        console.log('[CallContext] âœ… Ringtone source stopped');
      } catch (e) {
        console.log('[CallContext] Ringtone source already stopped or error:', e);
      }
    }
    
    // Clear any ringtone-related intervals (same pattern as waiting tone)
    if ((window as any).__ringtoneIntervalId) {
      console.log('[CallContext] Clearing stored ringtone interval');
      clearInterval((window as any).__ringtoneIntervalId);
      (window as any).__ringtoneIntervalId = null;
    }
    
    // Clear any ringtone-related timeouts
    if ((window as any).__ringtoneTimeout) {
      clearTimeout((window as any).__ringtoneTimeout);
      (window as any).__ringtoneTimeout = null;
      console.log('[CallContext] âœ… Ringtone timeout cleared');
    }
    
    console.log('[CallContext] âœ… RINGTONE CLEANUP COMPLETED');
  };

  // Function to create waiting tone (tuuutt sound)
  const createWaitingTone = () => {
    if (!audioContext) {
      const ctx = new (window.AudioContext || (window as any).webkitAudioContext)();
      setAudioContext(ctx);
      return ctx;
    }
    return audioContext;
  };

  // Function to play waiting tone
  const playWaitingTone = () => {
    try {
      const ctx = createWaitingTone();
      
      // Create oscillator for the "tuuutt" sound
      const oscillator = ctx.createOscillator();
      const gainNode = ctx.createGain();
      
      oscillator.connect(gainNode);
      gainNode.connect(ctx.destination);
      
      // Set frequency for waiting tone (400Hz - typical phone waiting tone)
      oscillator.frequency.value = 400;
      oscillator.type = 'sine';
      
      // Set volume
      gainNode.gain.value = 0.3;
      
      // Store oscillator globally for emergency cleanup
      (window as any).__currentOscillator = oscillator;
      (window as any).__currentGainNode = gainNode;
      
      // Start and stop the tone (0.8 seconds on, 0.8 seconds off)
      oscillator.start();
      oscillator.stop(ctx.currentTime + 0.8);
      
      // Clear references when oscillator ends
      oscillator.onended = () => {
        (window as any).__currentOscillator = null;
        (window as any).__currentGainNode = null;
      };
      
      console.log('[CallContext] Playing waiting tone - tuuutt');
    } catch (error) {
      console.error('[CallContext] Error playing waiting tone:', error);
    }
  };

  // Function to start waiting tone interval
  const startWaitingTone = () => {
    if (waitingToneInterval) {
      clearInterval(waitingToneInterval);
    }
    
    // Clear any existing global waiting tone interval
    if ((window as any).__waitingToneIntervalId) {
      clearInterval((window as any).__waitingToneIntervalId);
    }
    
    console.log('[CallContext] Starting waiting tone sequence');
    playWaitingTone(); // Play immediately
    
    // Then play every 2400ms (0.8s tone + 1.6s silence)
    const interval = setInterval(() => {
      playWaitingTone();
    }, 2400);
    
    setWaitingToneInterval(interval);
    // Store globally for emergency cleanup
    (window as any).__waitingToneIntervalId = interval;
  };

  // Function to stop waiting tone
  const stopWaitingTone = () => {
    console.log('[CallContext] ðŸ”‡ AGGRESSIVE AUDIO CLEANUP - STOPPING ALL AUDIO');
    
    // Clear the main waiting tone interval
    if (waitingToneInterval) {
      console.log('[CallContext] Clearing waiting tone interval');
      clearInterval(waitingToneInterval);
      setWaitingToneInterval(null);
    }
    
    // Stop current oscillator if playing
    if ((window as any).__currentOscillator) {
      try {
        console.log('[CallContext] Force stopping current oscillator');
        (window as any).__currentOscillator.stop();
        (window as any).__currentOscillator.disconnect();
        (window as any).__currentOscillator = null;
      } catch (e) {
        console.log('[CallContext] Oscillator already stopped or error:', e);
      }
    }
    
    // Disconnect current gain node
    if ((window as any).__currentGainNode) {
      try {
        console.log('[CallContext] Force disconnecting current gain node');
        (window as any).__currentGainNode.disconnect();
        (window as any).__currentGainNode = null;
      } catch (e) {
        console.log('[CallContext] Gain node already disconnected or error:', e);
      }
    }
    
    // NUCLEAR OPTION: Close ALL AudioContext instances in the browser
    try {
      console.log('[CallContext] ðŸ’¥ NUCLEAR AUDIO CLEANUP - Closing ALL AudioContext instances');
      
      // Close the main audio context
      if (audioContext) {
        audioContext.close();
        setAudioContext(null);
      }
      
      // Force close ALL possible AudioContext instances
      const contextTypes = ['AudioContext', 'webkitAudioContext', 'mozAudioContext'];
      contextTypes.forEach(contextType => {
        if ((window as any)[contextType]) {
          try {
            // Create a temporary context just to get access to close all
            const tempContext = new (window as any)[contextType]();
            tempContext.close();
            console.log(`[CallContext] âœ… Closed ${contextType}`);
          } catch (e) {
            console.log(`[CallContext] No ${contextType} to close`);
          }
        }
      });
      
      // Clear any global audio context references
      if ((window as any).globalAudioContext) {
        try {
          (window as any).globalAudioContext.close();
          delete (window as any).globalAudioContext;
        } catch (e) {
          console.log('[CallContext] Error closing global audio context');
        }
      }
      
    } catch (e) {
      console.log('[CallContext] Error in nuclear audio cleanup:', e);
    }
    
    // Store interval ID when creating waiting tone to clear it specifically
    if ((window as any).__waitingToneIntervalId) {
      console.log('[CallContext] Clearing stored waiting tone interval');
      clearInterval((window as any).__waitingToneIntervalId);
      (window as any).__waitingToneIntervalId = null;
    }
    
    // Clear ALL intervals and timeouts (extreme measure)
    try {
      console.log('[CallContext] ðŸ’¥ NUCLEAR INTERVAL CLEANUP - Clearing ALL intervals');
      const highestIntervalId = setInterval(() => {}, 9999) as unknown as number;
      for (let i = 1; i <= highestIntervalId; i++) {
        clearInterval(i);
      }
      clearInterval(highestIntervalId);
      
      const highestTimeoutId = setTimeout(() => {}, 9999) as unknown as number;
      for (let i = 1; i <= highestTimeoutId; i++) {
        clearTimeout(i);
      }
      clearTimeout(highestTimeoutId);
      
      console.log('[CallContext] âœ… Nuclear interval cleanup completed');
    } catch (e) {
      console.log('[CallContext] Error in nuclear interval cleanup:', e);
    }
    
    console.log('[CallContext] âœ… AGGRESSIVE AUDIO CLEANUP COMPLETED');
  };

  // Function to fetch participant names asynchronously
  const fetchParticipantNames = async (participantIds: number[], currentCall: CallState) => {
    try {
      const updatedParticipants = await Promise.all(
        participantIds.map(async (userId: number) => {
          if (userId === user?.id) {
            return {
              userId,
              userName: user?.callsign || user?.fullName || 'You',
              audioEnabled: true,
              videoEnabled: false,
              stream: null
            };
          } else {
            try {
              const response = await fetch(`/api/users/${userId}`);
              if (response.ok) {
                const userData = await response.json();
                return {
                  userId,
                  userName: userData.callsign || userData.fullName || `User ${userId}`,
                  audioEnabled: true,
                  videoEnabled: false,
                  stream: null
                };
              }
            } catch (error) {
              console.error('[CallContext] Error fetching user data:', error);
            }
            return {
              userId,
              userName: `User ${userId}`,
              audioEnabled: true,
              videoEnabled: false,
              stream: null
            };
          }
        })
      );

      // Update the call with proper names
      setActiveCall(prev => {
        if (prev && prev.callId === currentCall.callId) {
          return {
            ...prev,
            participants: updatedParticipants
          };
        }
        return prev;
      });

      console.log('[CallContext] Updated participant names:', updatedParticipants);
    } catch (error) {
      console.error('[CallContext] Error fetching participant names:', error);
    }
  };

  // Keep refs in sync with state
  useEffect(() => {
    activeCallRef.current = activeCall;
  }, [activeCall]);

  useEffect(() => {
    incomingCallRef.current = incomingCall;
  }, [incomingCall]);

  // Initialize ringtone audio once on mount
  useEffect(() => {
    if (ringtoneAudio) return;
    
    try {
      const audio = new Audio();
      const ringtoneData = 'data:audio/wav;base64,UklGRnoGAABXQVZFZm10IBAAAAABAAEAQB8AAEAfAAABAAgAZGF0YQoGAACBhYqFbF1fdJivrJBhNjVgodDbq2EcBj+a2+LDciUFLIHO8tiJNwgZaLvt559NEAxQp+PwtmMcBjiR1/LMeSwFJHfH8N2QQAoUXrTp66hVFApGn+DyvmwhC';
      
      audio.src = ringtoneData;
      audio.loop = true;
      audio.volume = 0.8;
      audio.preload = 'auto';
      audio.load();
      
      setRingtoneAudio(audio);
      console.log('[CallContext] Ringtone audio created successfully');
      
      // Setup user interaction for audio unlock
      const unlockAudio = () => {
        audio.play().then(() => {
          audio.pause();
          audio.currentTime = 0;
          console.log('[CallContext] Audio unlocked via user interaction');
        }).catch(() => {});
      };
      
      document.addEventListener('click', unlockAudio, { once: true });
      document.addEventListener('touchstart', unlockAudio, { once: true });
      
    } catch (error) {
      console.log('[CallContext] Could not create ringtone audio:', error);
    }
  }, []); // Only run once on mount

  // Simple WebSocket connection - no reconnection, just stable connection
  useEffect(() => {
    if (!user) return;

    console.log('[CallContext] Creating stable WebSocket for calls...');
    
    const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
    const wsUrl = `${protocol}//${window.location.host}/ws`;
    const websocket = new WebSocket(wsUrl);

    websocket.onopen = () => {
      console.log('[CallContext] WebSocket connected successfully for calls');
      setWs(websocket);
      
      // Set global WebSocket reference for GroupVideoCallSimple
      (window as any).__callWebSocket = websocket;
      
      // Listen for WebSocket message requests from GroupVideoCallSimple
      const handleSendWebSocketMessage = (event: CustomEvent) => {
        if (websocket && websocket.readyState === WebSocket.OPEN) {
          websocket.send(JSON.stringify(event.detail));
          console.log('[CallContext] Sent WebSocket message:', event.detail.type);
        }
      };
      
      window.addEventListener('send-websocket-message', handleSendWebSocketMessage as EventListener);
      
      // Authenticate user with WebSocket
      if (user?.id) {
        websocket.send(JSON.stringify({
          type: 'auth',
          payload: {
            userId: user.id
          }
        }));
        console.log(`[CallContext] Authenticated user ${user.id} with WebSocket`);
      }
      
      // Send any pending WebRTC offer
      if ((window as any).__pendingWebRTCOffer) {
        websocket.send(JSON.stringify((window as any).__pendingWebRTCOffer));
        console.log('[CallContext] âœ… Sent pending WebRTC offer after connection');
        (window as any).__pendingWebRTCOffer = null;
      }
      
      // Notify that WebSocket is ready
      window.dispatchEvent(new CustomEvent('websocket-ready', {
        detail: { websocket }
      }));
    };

    websocket.onmessage = (event) => {
      try {
        const message = JSON.parse(event.data);
        console.log('[CallContext] Received message:', message);
        
        // ðŸ” DEBUG: Add detailed message routing debugging
        console.log('[CallContext] ðŸ” DEBUG MESSAGE ROUTING:', {
          messageType: message.type,
          hasPayload: !!message.payload,
          messageKeys: Object.keys(message),
          routingToCase: message.type
        });
        
        // Debug log specifically for incoming group call messages
        if (message.type === 'incoming_group_call') {
          console.log('[CallContext] ðŸš¨ INCOMING GROUP CALL MESSAGE DETECTED - USER ID:', user?.id);
          console.log('[CallContext] ðŸš¨ MESSAGE PAYLOAD:', JSON.stringify(message, null, 2));
        }
        
        // Handle session termination (single session enforcement)
        if (message.type === 'session_terminated') {
          console.log('[CallContext] Session terminated:', message.payload);
          alert('Sesi Anda telah dihentikan karena Anda login dari perangkat lain');
          
          // Redirect to login page
          window.location.href = '/api/login';
          return;
        }
        
        // Handle real-time chat messages (for ChatRoom component) - CRITICAL SECTION
        if (message.type === 'new_message') {
          console.log('[CallContext] ðŸ”¥ REAL-TIME MESSAGE RECEIVED:', message.payload);
          console.log('[CallContext] ðŸ”¥ Message sender:', message.payload?.senderId);
          console.log('[CallContext] ðŸ”¥ Current user:', user?.id);
          console.log('[CallContext] ðŸ”¥ Message conversation ID:', message.payload?.conversationId);
          console.log('[CallContext] ðŸ”¥ Broadcasting to ChatRoom via custom event');
          
          // Play audio notification for new messages from other users
          if (message.payload?.senderId !== user?.id) {
            console.log('[CallContext] ðŸ”Š Playing audio notification for new message');
            playNewMessageSound();
          }
          
          // Dispatch multiple events for maximum reliability
          window.dispatchEvent(new CustomEvent('websocket-message', {
            detail: message
          }));
          
          // Also dispatch with different event name for fallback
          window.dispatchEvent(new CustomEvent('new-message-realtime', {
            detail: message
          }));
          
          // Dispatch event specifically for ChatList updates
          window.dispatchEvent(new CustomEvent('chatlist-update', {
            detail: message
          }));
          
          // Force immediate DOM update dispatch
          setTimeout(() => {
            window.dispatchEvent(new CustomEvent('force-message-refresh', {
              detail: message
            }));
          }, 100);
          
          console.log('[CallContext] ðŸ”¥ Multiple custom events dispatched successfully');
        }
        
        // Handle user status updates
        if (message.type === 'user_status') {
          console.log('[CallContext] Received user status update:', message.payload);
          window.dispatchEvent(new CustomEvent('websocket-message', {
            detail: message
          }));
        }
        
        // Handle call-specific messages - server uses payload wrapper
        switch (message.type) {

          case 'incoming_call':
            handleIncomingCall(message.payload || message);
            // Trigger call history update
            window.dispatchEvent(new CustomEvent('callHistoryUpdate', { 
              detail: { type: 'incoming_call', data: message.payload || message } 
            }));
            break;
          case 'incoming_group_call':
            console.log('[CallContext] Received incoming_group_call:', message.payload || message);
            handleIncomingGroupCall(message.payload || message);
            break;
          case 'call_accepted':
            handleCallAccepted(message.payload || message);
            break;
          case 'call_rejected':
            handleCallRejected(message.payload || message);
            break;
          case 'call_ended':
            handleCallEnded(message.payload || message);
            // Trigger call history update
            window.dispatchEvent(new CustomEvent('callHistoryUpdate', { 
              detail: { type: 'call_ended', data: message.payload || message } 
            }));
            break;
          case 'group_call_ended':
            handleGroupCallEnded(message.payload || message);
            break;
          case 'group_call_rejected':
            handleGroupCallRejected(message.payload || message);
            break;
          case 'group_call_user_left':
            handleGroupCallUserLeft(message);
            break;
          case 'group_call_participants_update':
            console.log('[CallContext] ðŸ”¥ GROUP_CALL_PARTICIPANTS_UPDATE received:', message);
            console.log('[CallContext] ðŸŽ¯ Participants in message:', message.payload?.participants);
            console.log('[CallContext] ðŸŽ¯ CallId in message:', message.payload?.callId);
            console.log('[CallContext] ðŸŽ¯ Full sync flag:', message.payload?.fullSync);
            handleGroupCallParticipantsUpdate(message);
            break;
          case 'group_update':
            handleGroupUpdate(message);
            break;
          case 'webrtc_ready':
            handleWebRTCReady(message.payload || message);
            break;
          case 'webrtc_offer':
            console.log('[CallContext] ðŸŽ¯ Processing webrtc_offer message:', message);
            handleWebRTCOffer(message.payload || message);
            break;
          case 'webrtc_answer':
            console.log('[CallContext] ðŸŽ¯ Processing webrtc_answer message:', message);
            handleWebRTCAnswer(message.payload || message);
            break;
          case 'webrtc_ice_candidate':
            handleWebRTCIceCandidate(message.payload || message);
            break;
          case 'group_webrtc_offer':
            // Forward group WebRTC offer to GroupVideoCallSimple component
            console.log('[CallContext] Forwarding group WebRTC offer:', message.payload || message);
            window.dispatchEvent(new CustomEvent('group-webrtc-offer', {
              detail: message.payload || message
            }));
            break;
          case 'group_webrtc_answer':
            // Forward group WebRTC answer to GroupVideoCallSimple component
            console.log('[CallContext] Forwarding group WebRTC answer:', message.payload || message);
            window.dispatchEvent(new CustomEvent('group-webrtc-answer', {
              detail: message.payload || message
            }));
            break;
          case 'group_webrtc_ice_candidate':
            // Forward group WebRTC ICE candidate to GroupVideoCallSimple component
            console.log('[CallContext] Forwarding group WebRTC ICE candidate:', message.payload || message);
            window.dispatchEvent(new CustomEvent('group-webrtc-ice-candidate', {
              detail: message.payload || message
            }));
            break;
          case 'group_participant_refresh':
            // Forward group participant refresh request to GroupVideoCallSimple component
            console.log('[CallContext] Forwarding group participant refresh:', message.payload || message);
            window.dispatchEvent(new CustomEvent('group-participant-refresh', {
              detail: message.payload || message
            }));
            break;
          case 'group_call_initiated':
            console.log('[CallContext] Group call initiated confirmation:', message.payload);
            // Optional: Show success message to initiator
            break;
          case 'group_call_no_participants':
            console.log('[CallContext] No participants available for group call:', message.payload);
            alert(message.payload?.message || 'Tidak ada anggota grup yang online saat ini.');
            break;
          case 'new_message':
            // Forward new message to Chat component for real-time updates
            console.log('[CallContext] Forwarding new message for Chat:', message.payload || message);
            window.dispatchEvent(new CustomEvent('new-message', {
              detail: message.payload || message
            }));
            break;
        }
      } catch (error) {
        // Ignore non-JSON messages (they might be for chat)
      }
    };

    websocket.onclose = () => {
      console.log('[CallContext] WebSocket disconnected');
      setWs(null);
    };

    websocket.onerror = (error) => {
      console.error('[CallContext] WebSocket error:', error);
      setWs(null);
    };

    return () => {
      if (websocket.readyState === WebSocket.OPEN || websocket.readyState === WebSocket.CONNECTING) {
        websocket.close();
      }
    };
  }, [user]);

  const handleIncomingCall = (message: any) => {
    console.log('[CallContext] Incoming call from:', message.fromUserName);
    
    // Play ringtone for incoming call with global reference pattern (same as waiting tone)
    const playRingtone = async () => {
      console.log('[CallContext] ðŸ”Š Starting ringtone for incoming call');
      try {
        if (ringtoneAudio) {
          // Store global reference for aggressive cleanup (same pattern as waiting tone)
          (window as any).__currentRingtone = ringtoneAudio;
          
          ringtoneAudio.currentTime = 0;
          console.log('[CallContext] Attempting to play ringtone...');
          
          // First try: Direct play
          try {
            await ringtoneAudio.play();
            console.log('[CallContext] âœ… Ringtone playing successfully');
            return;
          } catch (error) {
            console.log('[CallContext] Direct play failed, trying autoplay bypass techniques...');
          }
          
          // Second try: Reset audio and play with muted then unmuted
          ringtoneAudio.muted = true;
          await ringtoneAudio.play();
          ringtoneAudio.muted = false;
          console.log('[CallContext] âœ… Ringtone playing with mute bypass');
          return;
        }
      } catch (error) {
        console.log('[CallContext] âŒ HTML5 audio failed, trying Web Audio API...');
      }
      
      // Fallback 1: Advanced Web Audio API with user gesture simulation
      try {
        const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
        
        // Resume audio context if suspended
        if (audioContext.state === 'suspended') {
          await audioContext.resume();
        }
        
        // Create a more complex ringtone pattern
        const createRingtonePattern = () => {
          const duration = 3;
          const sampleRate = audioContext.sampleRate;
          const length = sampleRate * duration;
          const buffer = audioContext.createBuffer(1, length, sampleRate);
          const data = buffer.getChannelData(0);
          
          for (let i = 0; i < length; i++) {
            const time = i / sampleRate;
            let signal = 0;
            
            // Create ringtone pattern: ring ring pause ring ring
            if ((time % 1.5 < 0.3) || (time % 1.5 > 0.5 && time % 1.5 < 0.8)) {
              // Mix of frequencies for richer sound
              signal = (
                Math.sin(2 * Math.PI * 800 * time) * 0.3 +
                Math.sin(2 * Math.PI * 1000 * time) * 0.2 +
                Math.sin(2 * Math.PI * 600 * time) * 0.2
              );
              
              // Add envelope for smoother sound
              const envelope = Math.sin((time % 0.3) * Math.PI / 0.3);
              signal *= envelope;
            }
            
            data[i] = signal;
          }
          
          return buffer;
        };
        
        const buffer = createRingtonePattern();
        const source = audioContext.createBufferSource();
        const gainNode = audioContext.createGain();
        
        source.buffer = buffer;
        source.connect(gainNode);
        gainNode.connect(audioContext.destination);
        
        // Set volume
        gainNode.gain.setValueAtTime(0.7, audioContext.currentTime);
        
        // Play with looping
        source.loop = true;
        source.start();
        
        // Store source for cleanup (same pattern as waiting tone oscillator)
        (window as any).__ringtoneSource = source;
        (window as any).__currentRingtoneGainNode = gainNode;
        
        console.log('[CallContext] âœ… Web Audio API ringtone playing');
        return;
        
      } catch (webAudioError) {
        console.log('[CallContext] âŒ Web Audio API failed:', webAudioError);
      }
      
      // Fallback 2: Browser notification with sound
      try {
        if ('Notification' in window && Notification.permission === 'granted') {
          new Notification('ðŸ“ž Panggilan Masuk', {
            body: `${message.fromUserName} sedang menelpon`,
            icon: '/favicon.ico',
            tag: 'incoming-call',
            requireInteraction: true,
            silent: false
          });
          console.log('[CallContext] âœ… Notification displayed');
        } else if ('Notification' in window && Notification.permission !== 'denied') {
          // Request notification permission
          Notification.requestPermission().then(permission => {
            if (permission === 'granted') {
              new Notification('ðŸ“ž Panggilan Masuk', {
                body: `${message.fromUserName} sedang menelpon`,
                icon: '/favicon.ico',
                tag: 'incoming-call',
                requireInteraction: true
              });
            }
          });
        }
      } catch (notificationError) {
        console.log('[CallContext] âŒ Notification failed:', notificationError);
      }
      
      // Fallback 3: Simple oscillator beep
      try {
        const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
        const oscillator = audioContext.createOscillator();
        const gainNode = audioContext.createGain();
        
        oscillator.connect(gainNode);
        gainNode.connect(audioContext.destination);
        
        oscillator.frequency.setValueAtTime(800, audioContext.currentTime);
        gainNode.gain.setValueAtTime(0.3, audioContext.currentTime);
        
        oscillator.start();
        oscillator.stop(audioContext.currentTime + 0.5);
        
        console.log('[CallContext] âœ… Fallback beep played');
      } catch (beepError) {
        console.log('[CallContext] âŒ All audio methods failed:', beepError);
      }
    };
    
    playRingtone();
    
    // Create RTCPeerConnection for incoming call (offline/intranet mode)
    const peerConnection = new RTCPeerConnection({
      iceServers: [], // Empty array for local network only - no STUN/TURN servers
      iceTransportPolicy: 'all', // Allow both UDP and TCP
      bundlePolicy: 'max-bundle',
      rtcpMuxPolicy: 'require'
    });

    // Setup ICE candidate handling for incoming call
    peerConnection.onicecandidate = (event) => {
      if (event.candidate && ws && ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify({
          type: 'webrtc_ice_candidate',
          callId: message.callId,
          candidate: event.candidate
        }));
      }
    };

    // Setup remote stream handling for incoming call
    peerConnection.ontrack = (event) => {
      console.log('[CallContext] Received remote stream in incoming call setup');
      const remoteStream = event.streams[0];
      console.log('[CallContext] Remote stream tracks:', remoteStream.getTracks().length);
      
      // âš ï¸ PREVENT EARLY MEDIA LEAK - DO NOT store remote stream until call is answered
      console.log('[CallContext] ðŸ›¡ï¸ Early media protection - remote stream received but NOT stored until call answered');
      
      // Store stream in temporary variable for later use after answer
      (window as any).__pendingRemoteStream = remoteStream;
      
      // Mute all audio tracks immediately to prevent early media
      remoteStream.getAudioTracks().forEach(track => {
        track.enabled = false;
        console.log('[CallContext] ðŸ”‡ Audio track muted to prevent early media leak');
      });
      
      // Do NOT set remote stream yet - wait for call to be answered
      // setRemoteAudioStream(remoteStream); // REMOVED - akan di-set saat acceptCall
    };

    setIncomingCall({
      callId: message.callId,
      callType: message.callType,
      status: 'ringing',
      isIncoming: true,
      peerUserId: message.fromUserId,
      peerName: message.fromUserName,
      remoteStreams: new Map(),
      peerConnection,
      audioEnabled: false, // âš ï¸ PREVENT EARLY MEDIA - audio disabled until answered
      videoEnabled: false, // âš ï¸ PREVENT EARLY MEDIA - video disabled until answered
      isMuted: true, // âš ï¸ PREVENT EARLY MEDIA - muted until answered
    });

    // Process any pending WebRTC offers for this call
    setTimeout(() => {
      console.log('[CallContext] Checking for pending offers for callId:', message.callId);
      console.log('[CallContext] Total pending offers:', pendingOffers.current.length);
      console.log('[CallContext] Pending offer callIds:', pendingOffers.current.map(o => o.callId));
      
      const pendingOffer = pendingOffers.current.find(offer => offer.callId === message.callId);
      if (pendingOffer) {
        console.log('[CallContext] âœ… Found pending WebRTC offer for callId:', message.callId);
        console.log('[CallContext] Processing pending offer...');
        handleWebRTCOffer(pendingOffer);
        
        // Remove the processed offer from queue
        pendingOffers.current = pendingOffers.current.filter(offer => offer.callId !== message.callId);
        console.log('[CallContext] Removed processed offer, remaining pending:', pendingOffers.current.length);
      } else {
        console.log('[CallContext] âŒ No pending offer found for callId:', message.callId);
      }
    }, 100); // Small delay to ensure incoming call state is set
  };

  const handleCallAccepted = async (message: any) => {
    console.log('[CallContext] Call accepted, payload:', message);
    console.log('[CallContext] Current activeCall (state):', activeCall);
    console.log('[CallContext] Current activeCall (ref):', activeCallRef.current);
    console.log('[CallContext] Current incomingCall:', incomingCall);
    
    // FORCE STOP ALL WAITING TONES AND RINGTONES IMMEDIATELY
    console.log('[CallContext] FORCE STOPPING ALL RINGTONES - call accepted');
    stopAllRingtones();
    stopWaitingTone();
    
    // Additional comprehensive audio cleanup
    if (waitingToneInterval) {
      clearInterval(waitingToneInterval);
      setWaitingToneInterval(null);
      console.log('[CallContext] âœ… Waiting tone interval cleared');
    }
    
    // Stop any running audio contexts or oscillators
    if (audioContext) {
      try {
        audioContext.close();
        setAudioContext(null);
        console.log('[CallContext] âœ… Audio context closed');
      } catch (e) {
        console.log('[CallContext] Audio context already closed');
      }
    }
    
    // Use ref for stable call reference
    const currentActiveCall = activeCallRef.current || activeCall;
    
    if (currentActiveCall && currentActiveCall.peerConnection) {
      console.log('[CallContext] Updating call status to connected');
      
      // Stop ALL ringtone sources when call is accepted
      console.log('[CallContext] Stopping ALL ringtone sources - call accepted');
      
      // Force stop HTML5 Audio
      if (ringtoneAudio) {
        ringtoneAudio.pause();
        ringtoneAudio.currentTime = 0;
        ringtoneAudio.loop = false;
        ringtoneAudio.volume = 0;
        ringtoneAudio.src = '';
        ringtoneAudio.load();
        console.log('[CallContext] âœ… HTML5 ringtone force stopped');
      }
      
      // Force stop Web Audio API source
      if ((window as any).__ringtoneSource) {
        try {
          (window as any).__ringtoneSource.stop();
          (window as any).__ringtoneSource.disconnect();
          (window as any).__ringtoneSource = null;
          console.log('[CallContext] âœ… Web Audio API ringtone stopped');
        } catch (e) {
          console.log('[CallContext] Web Audio API source already stopped');
        }
      }
      
      // Close audio context completely
      if ((window as any).__audioContext) {
        try {
          (window as any).__audioContext.close();
          (window as any).__audioContext = null;
          console.log('[CallContext] âœ… Audio context closed');
        } catch (e) {
          console.log('[CallContext] Audio context already closed');
        }
      }
      
      // Clear any notifications
      if ('Notification' in window) {
        // Clear any existing notifications with our tag
        try {
          navigator.serviceWorker?.ready.then(registration => {
            registration.getNotifications({ tag: 'incoming-call' }).then(notifications => {
              notifications.forEach(notification => notification.close());
            });
          });
        } catch (e) {
          console.log('[CallContext] Could not clear notifications');
        }
      }

      // Update call status first
      setActiveCall(prevCall => {
        if (!prevCall) return prevCall;
        return {
          ...prevCall,
          status: 'connected' as const,
          startTime: new Date(),
        };
      });
      
      // Don't create offer here, wait for webrtc_ready signal from receiver
      console.log('[CallContext] Waiting for receiver to be ready for WebRTC...');
      
      // Add fallback mechanism - if no webrtc_ready signal received within 3 seconds, proceed anyway
      const fallbackTimeout = setTimeout(() => {
        console.log('[CallContext] âš ï¸ No WebRTC ready signal received, proceeding with fallback offer creation');
        const fallbackActiveCall = activeCallRef.current;
        if (fallbackActiveCall && fallbackActiveCall.peerConnection) {
          fallbackActiveCall.peerConnection!.createOffer()
            .then(offer => {
              fallbackActiveCall.peerConnection!.setLocalDescription(offer);
              if (ws && ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({
                  type: 'webrtc_offer',
                  callId: fallbackActiveCall.callId,
                  offer: offer
                }));
                console.log('[CallContext] âœ… Fallback WebRTC offer sent');
              }
            })
            .catch(error => {
              console.error('[CallContext] âŒ Fallback offer creation failed:', error);
            });
        }
      }, 3000);
      
      // Store timeout so we can clear it if webrtc_ready is received
      (window as any).__webrtcFallbackTimeout = fallbackTimeout;
    } else {
      console.error('[CallContext] âŒ No activeCall or peerConnection when call accepted');
      console.error('[CallContext] activeCall (state):', activeCall);
      console.error('[CallContext] activeCall (ref):', activeCallRef.current);
      console.error('[CallContext] incomingCall:', incomingCall);
    }
  };

  const handleWebRTCReady = async (message: any) => {
    console.log('[CallContext] Received WebRTC ready signal, payload:', message);
    console.log('[CallContext] Current activeCall (state):', activeCall);
    console.log('[CallContext] Current activeCall (ref):', activeCallRef.current);
    
    // Use ref for stable call reference
    const currentActiveCall = activeCallRef.current || activeCall;
    
    if (currentActiveCall && currentActiveCall.peerConnection) {
      console.log('[CallContext] âœ… Found activeCall and peerConnection for WebRTC ready');
      
      // Clear the fallback timeout since we received the ready signal
      if ((window as any).__webrtcFallbackTimeout) {
        clearTimeout((window as any).__webrtcFallbackTimeout);
        (window as any).__webrtcFallbackTimeout = null;
        console.log('[CallContext] âœ… Cleared fallback timeout - received ready signal');
      }
      
      // ðŸŽ¯ CRITICAL FIX 10: Ensure tracks are properly attached before offer creation
      try {
        console.log('[CallContext] ðŸ”„ ENHANCED: Preparing WebRTC offer with proper track validation...');
        
        // Validate that local tracks are properly attached BEFORE creating offer
        console.log('[CallContext] ðŸŽ¤ ENHANCED: Validating local track attachment before offer...');
        const senders = currentActiveCall.peerConnection.getSenders();
        console.log('[CallContext] ðŸ“Š Current senders before offer:', senders.length);
        
        let hasAudioTrack = false;
        senders.forEach((sender, index) => {
          const track = sender.track;
          console.log(`[CallContext] Sender ${index}:`, {
            hasTrack: !!track,
            kind: track?.kind,
            enabled: track?.enabled,
            readyState: track?.readyState,
            id: track?.id
          });
          
          if (track && track.kind === 'audio') {
            hasAudioTrack = true;
          }
        });
        
        if (!hasAudioTrack) {
          console.error('[CallContext] âŒ CRITICAL: No audio track attached to sender before offer creation!');
          console.error('[CallContext] ðŸ”§ CRITICAL FIX: Attempting to add local stream tracks now...');
          
          // Emergency fix: Add local stream tracks if they're missing
          if (currentActiveCall.localStream) {
            const audioTracks = currentActiveCall.localStream.getAudioTracks();
            console.log('[CallContext] ðŸš¨ Emergency track addition:', audioTracks.length, 'audio tracks available');
            
            audioTracks.forEach(track => {
              try {
                currentActiveCall.peerConnection.addTrack(track, currentActiveCall.localStream!);
                console.log('[CallContext] âœ… EMERGENCY: Added audio track to peer connection');
              } catch (error) {
                console.warn('[CallContext] âš ï¸ Track already added or error:', error);
              }
            });
          } else {
            console.error('[CallContext] âŒ CRITICAL: No local stream available for emergency track addition!');
            throw new Error('No audio track available for offer creation');
          }
        } else {
          console.log('[CallContext] âœ… ENHANCED: Audio track properly attached before offer creation');
        }
        
        // ðŸŽ¯ CRITICAL FIX 11: Enhanced offer creation with proper constraints
        console.log('[CallContext] ðŸ”„ ENHANCED: Creating offer with proper constraints...');
        const offerOptions: RTCOfferOptions = {
          offerToReceiveAudio: true,
          offerToReceiveVideo: currentActiveCall.callType === 'video',
          iceRestart: false
        };
        
        const offer = await currentActiveCall.peerConnection.createOffer(offerOptions);
        
        // ðŸŽ¯ CRITICAL FIX 12: Validate offer SDP for proper media sections
        console.log('[CallContext] ðŸ“‹ ENHANCED: Validating offer SDP...');
        const sdpLines = offer.sdp?.split('\n') || [];
        let hasAudioMedia = false;
        let hasAudioSendRecv = false;
        
        sdpLines.forEach(line => {
          if (line.startsWith('m=audio')) {
            hasAudioMedia = true;
            console.log('[CallContext] âœ… Found audio media section in offer');
          }
          if (line.includes('a=sendrecv') || line.includes('a=sendonly')) {
            hasAudioSendRecv = true;
            console.log('[CallContext] âœ… Found proper audio direction in offer:', line.trim());
          }
        });
        
        if (!hasAudioMedia) {
          console.error('[CallContext] âŒ CRITICAL: No audio media section in offer SDP!');
          throw new Error('Invalid offer SDP - no audio media section');
        }
        
        if (!hasAudioSendRecv) {
          console.warn('[CallContext] âš ï¸ WARNING: No sendrecv/sendonly direction in offer - may cause one-way audio');
        }
        
        await currentActiveCall.peerConnection.setLocalDescription(offer);
        console.log('[CallContext] âœ… ENHANCED: Local description set successfully with proper validation');

        // Try WebSocket first, fallback to HTTP API
        const offerData = {
          type: 'webrtc_offer',
          callId: currentActiveCall.callId,
          offer: offer
        };
        
        if (ws && ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify(offerData));
          console.log('[CallContext] âœ… Sent WebRTC offer via WebSocket');
        } else {
          console.log('[CallContext] âš ï¸ WebSocket not ready, using HTTP API fallback');
          
          // Use HTTP API as fallback
          try {
            const response = await fetch('/api/webrtc/offer', {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json'
              },
              body: JSON.stringify({
                callId: currentActiveCall.callId,
                targetUserId: currentActiveCall.peerUserId,
                offer: offer
              })
            });
            
            if (response.ok) {
              console.log('[CallContext] âœ… Sent WebRTC offer via HTTP API fallback');
            } else {
              throw new Error('HTTP API failed');
            }
          } catch (error) {
            console.error('[CallContext] âŒ Both WebSocket and HTTP API failed:', error);
            hangupCall();
          }
        }
      } catch (error) {
        console.error('[CallContext] âŒ Error creating WebRTC offer after receiver ready:', error);
      }
    } else {
      console.error('[CallContext] âŒ No activeCall or peerConnection when receiver ready');
      console.error('[CallContext] activeCall (state):', activeCall);
      console.error('[CallContext] activeCall (ref):', activeCallRef.current);
    }
  };

  const handleCallRejected = (message: any) => {
    console.log('[CallContext] Call rejected');
    
    // FORCE STOP ALL WAITING TONES AND RINGTONES IMMEDIATELY
    console.log('[CallContext] FORCE STOPPING ALL RINGTONES - call rejected');
    stopAllRingtones();
    stopWaitingTone();
    
    // Additional comprehensive audio cleanup for call rejected
    if (waitingToneInterval) {
      clearInterval(waitingToneInterval);
      setWaitingToneInterval(null);
      console.log('[CallContext] âœ… Waiting tone interval cleared on call rejected');
    }
    
    // Stop any running audio contexts or oscillators
    if (audioContext) {
      try {
        audioContext.close();
        setAudioContext(null);
        console.log('[CallContext] âœ… Audio context closed on call rejected');
      } catch (e) {
        console.log('[CallContext] Audio context already closed');
      }
    }
    
    // Clear stored interval ID
    if ((window as any).__waitingToneIntervalId) {
      console.log('[CallContext] Clearing stored waiting tone interval on call rejected');
      clearInterval((window as any).__waitingToneIntervalId);
      (window as any).__waitingToneIntervalId = null;
    }
    
    setActiveCall(null);
    console.log('[CallContext] Call rejected');
  };

  const handleIncomingGroupCall = async (message: any) => {
    console.log('[CallContext] ðŸ”” Incoming group call received:', message);
    const { callId, groupId, groupName, callType, fromUserId, fromUserName } = message.payload || message;

    console.log('[CallContext] Extracted call details:', { callId, groupId, groupName, callType });
    console.log('[CallContext] Current activeCall:', activeCall?.callId);

    // Skip if user is already in an active call
    if (activeCall) {
      console.log('[CallContext] User already in active call, ignoring incoming group call');
      return;
    }

    // Play notification sound for incoming group call
    console.log('[CallContext] ðŸ”Š Playing notification sound for incoming group call');
    
    try {
      // Play simple notification beep for incoming group call
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      const oscillator = audioContext.createOscillator();
      const gainNode = audioContext.createGain();
      
      oscillator.connect(gainNode);
      gainNode.connect(audioContext.destination);
      
      oscillator.frequency.setValueAtTime(800, audioContext.currentTime);
      gainNode.gain.setValueAtTime(0.3, audioContext.currentTime);
      
      oscillator.start();
      oscillator.stop(audioContext.currentTime + 0.5);
      
      console.log('[CallContext] âœ… Notification beep played successfully');
    } catch (error) {
      console.error('[CallContext] âŒ Error playing notification sound:', error);
      // Continue even if sound fails
    }

    console.log('[CallContext] ðŸ”§ Creating RTCPeerConnection for group call');
    
    let peerConnection;
    try {
      // Create RTCPeerConnection for group call (same as handleIncomingCall)
      peerConnection = new RTCPeerConnection({
        iceServers: [], // Empty array for local network only - no STUN/TURN servers
        iceTransportPolicy: 'all', // Allow both UDP and TCP
        bundlePolicy: 'max-bundle',
        rtcpMuxPolicy: 'require'
      });
      
      console.log('[CallContext] âœ… RTCPeerConnection created successfully');
    } catch (error) {
      console.error('[CallContext] âŒ Error creating RTCPeerConnection:', error);
      return; // Exit if can't create peer connection
    }

    try {
      // Setup ICE candidate handling for group call
      peerConnection.onicecandidate = (event) => {
        if (event.candidate && ws && ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify({
            type: 'webrtc_ice_candidate',
            callId: callId,
            candidate: event.candidate
          }));
        }
      };
      
      console.log('[CallContext] âœ… ICE candidate handler setup successfully');
    } catch (error) {
      console.error('[CallContext] âŒ Error setting up ICE candidate handler:', error);
    }

    // Pre-create local stream with video enabled for incoming group calls
    console.log('[CallContext] ðŸŽ¥ Pre-creating local stream for incoming group call...');
    let localStream: MediaStream | undefined;
    
    try {
      // Create stream with video enabled for video calls
      if (callType === 'video') {
        const constraints = {
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
            sampleRate: 48000,
            channelCount: 1
          },
          video: {
            width: { ideal: 640, max: 1280 },
            height: { ideal: 480, max: 720 },
            frameRate: { ideal: 24, max: 30 },
            facingMode: 'user'
          }
        };
        
        localStream = await navigator.mediaDevices.getUserMedia(constraints);
        console.log('[CallContext] âœ… Pre-created video+audio stream for incoming group call');
        
        // Ensure video track is enabled
        const videoTrack = localStream.getVideoTracks()[0];
        if (videoTrack) {
          videoTrack.enabled = true;
          console.log('[CallContext] âœ… Video track enabled in pre-created stream');
        }
      }
    } catch (error) {
      console.error('[CallContext] âš ï¸ Failed to pre-create stream, will create on accept:', error);
      // Continue without localStream, will be created on accept
    }

    // Show incoming group call modal instead of auto-joining
    console.log('[CallContext] ðŸ”¥ BEFORE setIncomingCall - current state:', incomingCall);
    
    try {
      // Create incoming call state exactly like handleIncomingCall does
      const groupCallState = {
        callId,
        callType,
        status: 'ringing',
        isIncoming: true,
        peerUserId: fromUserId,
        peerName: fromUserName,
        remoteStreams: new Map(),
        peerConnection,
        localStream, // Include pre-created stream if available
        audioEnabled: true,
        videoEnabled: callType === 'video',
        isMuted: false,
        isGroupCall: true,
        groupId,
        groupName,
        participants: []
      };
      
      console.log('[CallContext] ðŸ”¥ About to call setIncomingCall with:', groupCallState);
      setIncomingCall(groupCallState);
      console.log('[CallContext] ðŸ”¥ setIncomingCall called successfully');
      
    } catch (error) {
      console.error('[CallContext] âŒ Error setting incoming call state:', error);
      console.error('[CallContext] Error details:', error.message, error.stack);
    }
    
    console.log('[CallContext] ðŸ”¥ AFTER setIncomingCall - created incoming group call');
    console.log('[CallContext] ðŸŽ¯ Set incoming group call modal for:', groupName);
    
    // Debug: Check state immediately after setting
    setTimeout(() => {
      console.log('[CallContext] ðŸ” DEBUG: incomingCall state after 100ms:', incomingCall);
      console.log('[CallContext] ðŸ” DEBUG: incomingCallRef after 100ms:', incomingCallRef.current);
    }, 100);
  };

  const handleGroupCallEnded = (message: any) => {
    console.log('[CallContext] Group call ended:', message);
    
    // Stop ringtone
    if (ringtoneAudio) {
      ringtoneAudio.pause();
      ringtoneAudio.currentTime = 0;
    }

    // Clean up active call if it matches
    if (activeCall && activeCall.callId === message.payload.callId) {
      if (activeCall.localStream) {
        activeCall.localStream.getTracks().forEach(track => track.stop());
      }
      setActiveCall(null);
      setLocation('/chat');
    }

    // Clean up incoming call if it matches, but only if call was in progress
    // Don't clear incoming call modal if user hasn't responded yet
    if (incomingCall?.callId === message.payload.callId) {
      console.log('[CallContext] Group call ended - checking if should clear incoming call modal');
      console.log('[CallContext] Incoming call status:', incomingCall.status);
      
      // Only clear if call was actually in progress (accepted), not just ringing
      if (incomingCall.status !== 'ringing') {
        console.log('[CallContext] Clearing incoming call modal because call was in progress');
        setIncomingCall(null);
      } else {
        console.log('[CallContext] Keeping incoming call modal because call was only ringing (not accepted yet)');
      }
    }

    // Clear localStorage when group call ends
    localStorage.removeItem('activeGroupCall');
    console.log('[CallContext] Cleared group call data from localStorage');
  };

  const handleGroupCallRejected = (message: any) => {
    console.log('[CallContext] ðŸš« Group call rejected by user:', message);
    const { callId, groupId, rejectedByUserId, rejectedByUserName } = message;
    
    // Show notification to other participants that someone rejected the call
    console.log(`[CallContext] User ${rejectedByUserName} (${rejectedByUserId}) rejected group call ${callId}`);
    
    // Optional: Show toast notification
    if (typeof window !== 'undefined' && window.dispatchEvent) {
      window.dispatchEvent(new CustomEvent('show-toast', {
        detail: {
          title: 'Group Call Update',
          description: `${rejectedByUserName} menolak panggilan grup`,
          variant: 'warning'
        }
      }));
    }
    
    // Don't automatically end the call, let other participants continue
    // The call should only end when all participants have rejected or left
    console.log('[CallContext] âœ… Group call rejection handled - other participants can still join');
  };

  const handleGroupCallUserLeft = (message: any) => {
    console.log('[CallContext] User left group call:', message);
    const { userId, roomId, callType } = message;
    
    // Get current active call
    const currentActiveCall = activeCallRef.current;
    
    if (currentActiveCall && currentActiveCall.isGroupCall && currentActiveCall.groupId === roomId) {
      console.log(`[CallContext] Removing user ${userId} from group call participants`);
      
      // Remove the user from participants list  
      const currentParticipants = currentActiveCall.participants || [];
      const updatedParticipants = currentParticipants.filter((participant: any) => 
        typeof participant === 'object' ? participant.userId !== userId : participant !== userId
      );
      
      const updatedCall = {
        ...currentActiveCall,
        participants: updatedParticipants
      };
      
      setActiveCall(updatedCall);
      console.log(`[CallContext] Updated participants after user ${userId} left:`, updatedParticipants);
      
      // Don't end call if current user is still in it - let them decide when to hang up
      // Only end if there are literally no participants (shouldn't happen in normal flow)
      if (updatedParticipants.length === 0) {
        console.log('[CallContext] No participants left, ending group call');
        handleGroupCallEnded({ payload: { callId: currentActiveCall.callId } });
      } else {
        console.log(`[CallContext] ${updatedParticipants.length} participant(s) remaining, call continues`);
      }
    } else {
      console.log('[CallContext] No matching active group call found for user left event');
    }
  };

  // Handle group update notifications for real-time cache invalidation
  const handleGroupUpdate = (message: any) => {
    console.log('[CallContext] Group update received:', message);
    
    if (message.payload) {
      const { groupId, updateType } = message.payload;
      console.log(`[CallContext] Processing group update: ${updateType} for group ${groupId}`);
      
      // Trigger a custom event that GroupManagement can listen to
      window.dispatchEvent(new CustomEvent('group-update', {
        detail: {
          groupId,
          updateType,
          data: message.payload.data
        }
      }));
    }
  };

  const handleGroupCallParticipantsUpdate = (message: any) => {
    console.log('[CallContext] ðŸ”¥ Group call participants update:', message);
    console.log('[CallContext] ðŸ”¥ Message payload:', message.payload);
    const { callId, participants, newParticipant, fullSync, participantData } = message.payload;
    
    // Use ref to get current activeCall state to avoid race conditions
    const currentActiveCall = activeCallRef.current;
    console.log('[CallContext] Comparing callIds - message:', callId, 'activeCall:', currentActiveCall?.callId);
    console.log('[CallContext] ActiveCall groupId:', currentActiveCall?.groupId);
    
    // Also check localStorage for active group call if activeCall is null
    let groupCallToUpdate = currentActiveCall;
    if (!groupCallToUpdate || !groupCallToUpdate.isGroupCall) {
      const storedCall = localStorage.getItem('activeGroupCall');
      if (storedCall) {
        try {
          const storedData = JSON.parse(storedCall);
          console.log('[CallContext] Using stored group call data:', storedData);
          
          // Create a minimal call state from localStorage
          groupCallToUpdate = {
            callId: storedData.callId,
            callType: storedData.callType,
            status: 'connected',
            isIncoming: false,
            groupId: storedData.groupId,
            groupName: storedData.groupName,
            isGroupCall: true,
            participants: [],
            remoteStreams: new Map(),
            audioEnabled: true,
            videoEnabled: storedData.callType === 'video',
            isMuted: false
          };
        } catch (error) {
          console.error('[CallContext] Error parsing stored call data:', error);
        }
      }
    }
    
    // For group calls, match by groupId rather than exact callId since the server may use different callIds
    if (groupCallToUpdate && groupCallToUpdate.isGroupCall) {
      // Extract groupId from the callId (format: group_call_timestamp_groupId_userId)
      const callIdParts = callId.split('_');
      const messageGroupId = callIdParts[3]; // Fourth part is groupId (index 3)
      const activeGroupId = String(groupCallToUpdate.groupId);
      
      console.log('[CallContext] Extracted groupIds - message:', messageGroupId, 'active:', activeGroupId);
      console.log('[CallContext] CallId parts:', callIdParts);
      
      if (messageGroupId === activeGroupId) {
        console.log('[CallContext] Group IDs match, updating participants:', participants);
        
        // ðŸ”¥ ENHANCED: Handle detailed participant data for new members
        let processedParticipants = participants;
        let isDetailedParticipantData = false;
        
        console.log('[CallContext] ðŸŽ¯ Processing participant update - fullSync:', fullSync, 'participantData:', participantData);
        console.log('[CallContext] ðŸŽ¯ Participants array length:', participants?.length, 'ParticipantData array length:', participantData?.length);
        
        // Check if we have detailed participant data from server
        if (participantData && participantData.length > 0) {
          // Use the detailed participant data provided by server
          processedParticipants = participantData;
          isDetailedParticipantData = true;
          console.log('[CallContext] ðŸ”¥ Using detailed participant data from server:', participantData);
          console.log('[CallContext] ðŸ”¥ Detailed participant sample:', participantData[0]);
        } else if (participants && participants.length > 0 && typeof participants[0] === 'object' && participants[0].userId) {
          // Already in participant object format with detailed data
          isDetailedParticipantData = true;
          console.log('[CallContext] Received detailed participant data:', participants);
          // Keep the detailed format for better participant info
          processedParticipants = participants;
        } else {
          // Simple ID array, convert to participant objects
          processedParticipants = participants.map((id: any) => ({
            userId: id,
            userName: `User ${id}`,
            audioEnabled: true,
            videoEnabled: groupCallToUpdate.callType === 'video'
          }));
        }
        
        // Remove duplicates from participants list (by userId)
        const uniqueParticipants: any[] = [];
        const seenIds = new Set();
        
        for (const participant of processedParticipants) {
          const userId = typeof participant === 'object' ? participant.userId : participant;
          if (!seenIds.has(userId)) {
            seenIds.add(userId);
            uniqueParticipants.push(participant);
          }
        }
        
        console.log('[CallContext] Unique participants after deduplication:', uniqueParticipants);
        
        // ðŸš€ CRITICAL: For new members with fullSync, ensure they get complete participant data
        if (message.payload.fullSync && message.payload.isNewMember) {
          console.log('[CallContext] ðŸ”¥ Full sync for new member - ensuring complete participant data');
          
          // Update active call with complete participant data
          const updatedCall = {
            ...groupCallToUpdate,
            participants: uniqueParticipants
          };
          
          setActiveCall(updatedCall);
          activeCallRef.current = updatedCall;
          
          // Also trigger custom event for GroupCall component to refresh
          window.dispatchEvent(new CustomEvent('participant-data-updated', {
            detail: {
              callId: callId,
              participants: uniqueParticipants,
              isNewMember: true,
              fullSync: true
            }
          }));
          
          console.log('[CallContext] ðŸŽ¯ Full sync complete - participant data updated for new member');
          return;
        }
        
        // Handle regular participant updates (not full sync)
        console.log('[CallContext] Processing regular participant update (not full sync)');
        
        // Still need to process participant names for regular updates
        if (typeof uniqueParticipants[0] === 'number' || typeof uniqueParticipants[0] === 'string') {
          console.log('[CallContext] Converting participant IDs to objects for regular update');
          
          // Call the same user names processing for regular updates
          fetch('/api/all-users').then(response => response.json()).then(allUsers => {
            const userMap = new Map();
            allUsers.forEach((user: any) => {
              userMap.set(user.id, user.callsign || user.fullName || `User ${user.id}`);
            });
            
            // Convert participant IDs to participant objects
            const participantObjects = uniqueParticipants.map((participantId: any) => ({
              userId: Number(participantId),
              userName: userMap.get(Number(participantId)) || `User ${participantId}`,
              audioEnabled: true,
              videoEnabled: groupCallToUpdate.callType === 'video',
              stream: null
            }));
            
            console.log('[CallContext] ðŸ“‹ Regular update - created participant objects:', participantObjects);
            
            setActiveCall(prev => {
              if (prev && prev.isGroupCall) {
                const updatedCall = {
                  ...prev,
                  participants: participantObjects
                };
                activeCallRef.current = updatedCall;
                return updatedCall;
              }
              return prev;
            });
            
            // Also dispatch event for immediate UI updates
            window.dispatchEvent(new CustomEvent('participant-data-updated', {
              detail: {
                callId: callId,
                participants: participantObjects,
                isNewMember: false,
                fullSync: false
              }
            }));
            
          }).catch(error => {
            console.error('[CallContext] Error in regular update:', error);
            // Fallback without names
            setActiveCall(prev => {
              if (prev && prev.isGroupCall) {
                const updatedCall = {
                  ...prev,
                  participants: uniqueParticipants.map((id: any) => ({
                    userId: Number(id),
                    userName: `User ${id}`,
                    audioEnabled: true,
                    videoEnabled: groupCallToUpdate.callType === 'video',
                    stream: null
                  }))
                };
                activeCallRef.current = updatedCall;
                return updatedCall;
              }
              return prev;
            });
          });
        } else {
          // Participants are already in object format
          setActiveCall(prev => {
            if (prev && prev.isGroupCall) {
              const updatedCall = {
                ...prev,
                participants: uniqueParticipants
              };
              activeCallRef.current = updatedCall;
              return updatedCall;
            }
            return prev;
          });
          
          // Also dispatch event for immediate UI updates
          window.dispatchEvent(new CustomEvent('participant-data-updated', {
            detail: {
              callId: callId,
              participants: uniqueParticipants,
              isNewMember: false,
              fullSync: false
            }
          }));
        }
        
        // Clear any pending updates since we processed this one
        localStorage.removeItem('pendingParticipantUpdate');
        
        // ðŸ”¥ ENHANCED: Dispatch detailed participant update event for fullSync
        if (fullSync && participantData) {
          console.log('[CallContext] ðŸ”¥ Dispatching participant-data-updated event for fullSync');
          window.dispatchEvent(new CustomEvent('participant-data-updated', {
            detail: {
              callId: callId,
              participants: processedParticipants,
              isNewMember: false,
              fullSync: true,
              participantData: participantData
            }
          }));
        }
      } else {
        console.log('[CallContext] Group IDs do not match');
      }
    } else {
      console.log('[CallContext] No active group call found for participant update');
      // Store pending participant update for processing when activeCall becomes available
      localStorage.setItem('pendingParticipantUpdate', JSON.stringify({
        type: 'group_call_participants_update',
        payload: {
          callId,
          participants,
          participantData,
          fullSync,
          timestamp: Date.now()
        }
      }));
      console.log('[CallContext] Stored pending participant update for later processing');
    }
  };

  const handleInitiateGroupWebRTC = (message: any) => {
    console.log('[CallContext] ðŸš€ Group WebRTC initiation received:', message);
    const { callId, participants, forceInit, newMember } = message.payload;
    
    // Get current active call
    const currentActiveCall = activeCallRef.current || activeCall;
    
    // ðŸ” DEBUG: Add detailed logging to understand the issue
    console.log('[CallContext] ðŸ” DEBUG handleInitiateGroupWebRTC:', {
      currentActiveCall: currentActiveCall ? {
        callId: currentActiveCall.callId,
        isGroupCall: currentActiveCall.isGroupCall,
        callType: currentActiveCall.callType,
        status: currentActiveCall.status
      } : null,
      activeCallFromState: activeCall ? {
        callId: activeCall.callId,
        isGroupCall: activeCall.isGroupCall,
        callType: activeCall.callType
      } : null,
      messageCallId: callId,
      messageParticipants: participants
    });
    
    if (!currentActiveCall || !currentActiveCall.isGroupCall) {
      console.log('[CallContext] âŒ No active group call found for WebRTC initiation');
      console.log('[CallContext] âŒ DEBUG: currentActiveCall exists:', !!currentActiveCall);
      console.log('[CallContext] âŒ DEBUG: isGroupCall:', currentActiveCall?.isGroupCall);
      return;
    }
    
    console.log('[CallContext] ðŸŽ¯ Auto-triggering WebRTC setup for group call participants:', participants);
    
    // Dispatch event to GroupVideoCall to auto-start WebRTC connections
    window.dispatchEvent(new CustomEvent('auto-initiate-webrtc', {
      detail: {
        callId,
        participants,
        forceInit,
        newMember,
        activeCall: currentActiveCall
      }
    }));
    
    // ðŸš€ CRITICAL FIX: Handle server-forced WebRTC initiation for new members
    if (forceInit && newMember) {
      console.log(`[CallContext] ðŸ”„ Server forced WebRTC initiation for new member ${newMember}`);
      
      // Trigger immediate WebRTC reconnection
      setTimeout(() => {
        window.dispatchEvent(new CustomEvent('force-webrtc-reconnect', { 
          detail: {
            callId,
            participants,
            newMember,
            forceInit: true,
            fromServer: true
          }
        }));
      }, 100);
    }
    
    console.log('[CallContext] âœ… WebRTC auto-initiation event dispatched');
  };

  const handleGroupCallNoParticipants = (message: any) => {
    console.log('[CallContext] âš ï¸ No participants available for group call:', message);
    const { callId, message: warningMessage } = message.payload;
    
    // Show warning to user that no group members are online
    if (warningMessage) {
      // Create a toast notification or alert
      if (typeof window !== 'undefined' && window.dispatchEvent) {
        window.dispatchEvent(new CustomEvent('show-toast', {
          detail: {
            title: 'Group Call Notice',
            description: warningMessage,
            variant: 'warning'
          }
        }));
      }
    }
    
    // End the current call since no one is available
    setTimeout(() => {
      if (activeCallRef.current && activeCallRef.current.callId === callId) {
        console.log('[CallContext] Auto-ending group call due to no participants');
        hangupCall();
      }
    }, 3000); // Wait 3 seconds to show the message
  };

  const handleCallEnded = (message: any) => {
    console.log('[CallContext] âŒ Call ended unexpectedly, payload:', message);
    console.log('[CallContext] Current activeCall status:', activeCall?.status);
    console.log('[CallContext] Current incomingCall status:', incomingCall?.status);
    
    // FORCE STOP ALL WAITING TONES AND RINGTONES IMMEDIATELY
    console.log('[CallContext] FORCE STOPPING ALL RINGTONES - call ended');
    stopWaitingTone();
    
    // Additional comprehensive audio cleanup for call ended
    if (waitingToneInterval) {
      clearInterval(waitingToneInterval);
      setWaitingToneInterval(null);
      console.log('[CallContext] âœ… Waiting tone interval cleared on call ended');
    }
    
    // Stop any running audio contexts or oscillators
    if (audioContext) {
      try {
        audioContext.close();
        setAudioContext(null);
        console.log('[CallContext] âœ… Audio context closed on call ended');
      } catch (e) {
        console.log('[CallContext] Audio context already closed');
      }
    }
    
    // Stop ALL ringtone sources immediately
    console.log('[CallContext] Stopping ALL ringtone sources - call ended');
    try {
      // Stop HTML5 audio
      if (ringtoneAudio) {
        ringtoneAudio.pause();
        ringtoneAudio.currentTime = 0;
      }
      
      // Stop Web Audio API
      if ((window as any).__ringtoneSource) {
        (window as any).__ringtoneSource.stop();
        (window as any).__ringtoneSource = null;
      }
      
      // Clear fallback timeout
      if ((window as any).__webrtcFallbackTimeout) {
        clearTimeout((window as any).__webrtcFallbackTimeout);
        (window as any).__webrtcFallbackTimeout = null;
      }
    } catch (error) {
      console.log('[CallContext] Error stopping audio sources:', error);
    }
    
    // CRITICAL FIX: Only clear states for actual ongoing calls, NOT incoming calls
    // Check if this call ended event is for the current active call
    const callId = message.callId;
    
    if (activeCall && activeCall.callId === callId) {
      console.log('[CallContext] âœ… Ending active call:', callId);
      setActiveCall(null);
      
      // Clean up media streams
      if (activeCall.localStream) {
        activeCall.localStream.getTracks().forEach(track => track.stop());
      }
    } else {
      console.log('[CallContext] âš ï¸ Call ended event for non-active call:', callId, 'current activeCall:', activeCall?.callId);
    }
    
    // CRITICAL: Do NOT clear incoming call if it's still ringing
    // Only clear incoming call if the ended call matches AND it was accepted (not ringing)
    if (incomingCall && incomingCall.callId === callId && incomingCall.status !== 'ringing') {
      console.log('[CallContext] âœ… Clearing incoming call because it was accepted and then ended');
      setIncomingCall(null);
    } else if (incomingCall && incomingCall.callId === callId && incomingCall.status === 'ringing') {
      console.log('[CallContext] ðŸš« NOT clearing incoming call - still ringing, user hasn\'t responded yet');
    } else {
      console.log('[CallContext] âš ï¸ Call ended event does not match incoming call, or no incoming call exists');
    }
  };

  const handleWebRTCOffer = async (message: any) => {
    console.log('[CallContext] ðŸ”„ ENHANCED: Received WebRTC offer for callId:', message.callId);
    
    // Use ref for more stable reference
    const currentActiveCall = activeCallRef.current || activeCall;
    const currentIncomingCall = incomingCallRef.current || incomingCall;
    const currentCall = currentActiveCall || currentIncomingCall;
    
    console.log('[CallContext] Current activeCall:', currentActiveCall?.callId);
    console.log('[CallContext] Current incomingCall:', currentIncomingCall?.callId);
    console.log('[CallContext] Looking for callId:', message.callId);
    
    // If no call exists yet or callId doesn't match, queue the offer
    if (!currentCall || !currentCall.peerConnection || currentCall.callId !== message.callId) {
      console.log('[CallContext] No matching call found, queuing WebRTC offer for callId:', message.callId);
      
      // Initialize pending offers array if not exists
      if (!pendingOffers.current) {
        pendingOffers.current = [];
      }
      
      // Store the offer for later processing
      pendingOffers.current.push(message);
      console.log('[CallContext] Queued offer, total pending:', pendingOffers.current.length);
      return;
    }

    try {
      console.log('[CallContext] ðŸ”„ ENHANCED: Processing WebRTC offer with improved validation...');
      
      // ðŸŽ¯ CRITICAL FIX 1: Validate and log transceiver state BEFORE setting remote description
      console.log('[CallContext] ðŸ“Š Pre-offer transceiver validation:');
      const transceivers = currentCall.peerConnection.getTransceivers();
      transceivers.forEach((transceiver, index) => {
        console.log(`[CallContext] Transceiver ${index}:`, {
          direction: transceiver.direction,
          currentDirection: transceiver.currentDirection,
          mid: transceiver.mid,
          sender: {
            track: transceiver.sender.track?.kind,
            enabled: transceiver.sender.track?.enabled
          },
          receiver: {
            track: transceiver.receiver.track?.kind,
            enabled: transceiver.receiver.track?.enabled
          }
        });
      });
      
      // ðŸŽ¯ CRITICAL FIX 2: Perfect negotiation pattern with rollback
      const isOfferCollision = 
        (currentCall.peerConnection.signalingState !== 'stable') &&
        (currentCall.peerConnection.localDescription?.type === 'offer');
      
      if (isOfferCollision) {
        console.log('[CallContext] ðŸ”„ ENHANCED: Offer collision detected, implementing rollback strategy');
        
        // Determine if we should rollback (polite peer)
        const isPolitePeer = currentCall.isIncoming; // Receiver is polite
        
        if (isPolitePeer) {
          console.log('[CallContext] Acting as polite peer - rolling back local offer');
          await currentCall.peerConnection.setLocalDescription({type: 'rollback'} as RTCSessionDescriptionInit);
        } else {
          console.log('[CallContext] Acting as impolite peer - ignoring remote offer');
          return;
        }
      }
      
      await currentCall.peerConnection.setRemoteDescription(new RTCSessionDescription(message.offer));
      console.log('[CallContext] âœ… ENHANCED: Remote description set successfully');
      
      // ðŸŽ¯ CRITICAL FIX 3: Enhanced receiver-side transceiver validation and direction enforcement
      console.log('[CallContext] ðŸ“Š RECEIVER: Post-offer transceiver validation with direction enforcement:');
      const updatedTransceivers = currentCall.peerConnection.getTransceivers();
      let hasProperAudioTransceiver = false;
      let audioReceiverTransceiver: RTCRtpTransceiver | null = null;
      
      updatedTransceivers.forEach((transceiver, index) => {
        const direction = transceiver.direction;
        const currentDirection = transceiver.currentDirection;
        
        console.log(`[CallContext] RECEIVER: Transceiver ${index}:`, {
          direction,
          currentDirection,
          mid: transceiver.mid,
          kind: transceiver.receiver.track?.kind,
          receiverTrack: {
            id: transceiver.receiver.track?.id,
            enabled: transceiver.receiver.track?.enabled,
            muted: transceiver.receiver.track?.muted,
            readyState: transceiver.receiver.track?.readyState
          }
        });
        
        // ðŸŽ¯ CRITICAL: Focus on audio receiver configuration
        if (transceiver.receiver.track?.kind === 'audio') {
          audioReceiverTransceiver = transceiver;
          
          // ðŸŽ¯ CRITICAL FIX: Enforce proper receiver direction
          if (direction !== 'sendrecv' && direction !== 'recvonly') {
            console.warn(`[CallContext] ðŸ”§ RECEIVER: Correcting audio transceiver direction from ${direction} to sendrecv`);
            transceiver.direction = 'sendrecv';
          }
          
          // ðŸŽ¯ CRITICAL FIX: Ensure receiver track is enabled for RTP processing
          if (transceiver.receiver.track && !transceiver.receiver.track.enabled) {
            console.warn('[CallContext] ðŸ”§ RECEIVER: Enabling disabled audio receiver track');
            transceiver.receiver.track.enabled = true;
          }
          
          if (direction === 'sendrecv' || direction === 'recvonly') {
            hasProperAudioTransceiver = true;
            console.log('[CallContext] âœ… RECEIVER: Proper audio receiver transceiver validated');
          }
        }
      });
      
      if (!hasProperAudioTransceiver || !audioReceiverTransceiver) {
        console.error('[CallContext] âŒ CRITICAL: No proper audio receiver transceiver found after offer');
        throw new Error('No proper audio receiver transceiver configured');
      }
      
      // ðŸŽ¯ CRITICAL FIX: Enhanced receiver track validation and auto-repair
      console.log('[CallContext] ðŸ”§ RECEIVER: Implementing receiver track validation and auto-repair...');
      const audioReceiver = audioReceiverTransceiver.receiver;
      const audioReceiverTrack = audioReceiver.track;
      
      if (audioReceiverTrack) {
        console.log('[CallContext] ðŸ“Š RECEIVER: Audio receiver track state:', {
          id: audioReceiverTrack.id,
          enabled: audioReceiverTrack.enabled,
          muted: audioReceiverTrack.muted,
          readyState: audioReceiverTrack.readyState
        });
        
        // ðŸŽ¯ CRITICAL: Set up receiver track event monitoring for auto-repair
        audioReceiverTrack.addEventListener('mute', () => {
          console.warn('[CallContext] âš ï¸ RECEIVER: Audio receiver track muted - potential RTP issue');
          setTimeout(() => {
            if (audioReceiverTrack.muted) {
              console.log('[CallContext] ðŸ”§ RECEIVER: Attempting to unmute receiver track');
              // Force re-enable if still muted after 1 second
              audioReceiverTrack.enabled = false;
              setTimeout(() => { audioReceiverTrack.enabled = true; }, 100);
            }
          }, 1000);
        });
        
        audioReceiverTrack.addEventListener('unmute', () => {
          console.log('[CallContext] âœ… RECEIVER: Audio receiver track unmuted - RTP flow restored');
        });
        
        audioReceiverTrack.addEventListener('ended', () => {
          console.error('[CallContext] âŒ RECEIVER: Audio receiver track ended - attempting recovery');
          // Trigger receiver track recovery mechanism
          setTimeout(() => {
            if (currentCall?.peerConnection && audioReceiverTransceiver) {
              console.log('[CallContext] ðŸ”§ RECEIVER: Attempting transceiver recovery after track end');
              try {
                // Force renegotiation to recover receiver track
                audioReceiverTransceiver.direction = 'sendrecv';
              } catch (error) {
                console.error('[CallContext] âŒ Error during receiver track recovery:', error);
              }
            }
          }, 500);
        });
        
        // ðŸŽ¯ CRITICAL: Force enable receiver track for RTP processing
        audioReceiverTrack.enabled = true;
        console.log('[CallContext] âœ… RECEIVER: Audio receiver track force-enabled for RTP processing');
      }
      
      // Process any pending ICE candidates for this call now that remote description is set
      const callCandidates = pendingIceCandidates.current.filter(item => item.callId === currentCall.callId);
      if (callCandidates.length > 0) {
        console.log('[CallContext] Processing', callCandidates.length, 'pending ICE candidates for callId:', currentCall.callId);
        for (const item of callCandidates) {
          try {
            await currentCall.peerConnection.addIceCandidate(new RTCIceCandidate(item.candidate));
            console.log('[CallContext] âœ… Added pending ICE candidate');
          } catch (error) {
            console.error('[CallContext] Error adding pending ICE candidate:', error);
          }
        }
        // Remove processed candidates from queue
        pendingIceCandidates.current = pendingIceCandidates.current.filter(item => item.callId !== currentCall.callId);
      }
      
      // ðŸŽ¯ CRITICAL FIX: Start receiver-side RTP monitoring after remote description is set
      console.log('[CallContext] ðŸ”„ RECEIVER: Starting enhanced RTP flow monitoring for receiver side');
      startReceiverRTPMonitoring(currentCall.peerConnection, currentCall.callId);
      
      const answer = await currentCall.peerConnection.createAnswer();
      await currentCall.peerConnection.setLocalDescription(answer);
      console.log('[CallContext] âœ… ENHANCED: Local description (answer) set successfully');

      // Send answer back - try WebSocket first, fallback to HTTP API
      if (ws && ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify({
          type: 'webrtc_answer',
          callId: currentCall.callId,
          answer: answer
        }));
        console.log('[CallContext] âœ… Sent WebRTC answer via WebSocket');
      } else {
        console.log('[CallContext] âš ï¸ WebSocket not ready for answer, using HTTP API fallback');
        
        // Use HTTP API as fallback for answer
        try {
          const response = await fetch('/api/webrtc/answer', {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json'
            },
            body: JSON.stringify({
              callId: currentCall.callId,
              targetUserId: currentCall.peerUserId,
              answer: answer
            })
          });
          
          if (response.ok) {
            console.log('[CallContext] âœ… Sent WebRTC answer via HTTP API fallback');
          } else {
            throw new Error('HTTP API failed for answer');
          }
        } catch (error) {
          console.error('[CallContext] âŒ Failed to send WebRTC answer:', error);
        }
      }
    } catch (error) {
      console.error('[CallContext] Error handling WebRTC offer:', error);
    }
  };

  const handleWebRTCAnswer = async (message: any) => {
    console.log('[CallContext] ðŸ“¡ ENHANCED: Received WebRTC answer for callId:', message.callId);
    
    // Use ref for stable call reference
    const currentActiveCall = activeCallRef.current || activeCall;
    console.log('[CallContext] Current activeCall for answer:', currentActiveCall?.callId);
    
    if (!currentActiveCall || !currentActiveCall.peerConnection) {
      console.error('[CallContext] âŒ No activeCall or peerConnection for answer');
      return;
    }

    if (currentActiveCall.callId !== message.callId) {
      console.error('[CallContext] âŒ CallId mismatch for answer. Expected:', currentActiveCall.callId, 'Got:', message.callId);
      return;
    }

    try {
      console.log('[CallContext] ðŸ“¡ ENHANCED: Setting remote description (answer) with validation');
      
      // ðŸŽ¯ CRITICAL FIX 4: Validate signaling state before setting answer
      const signalingState = currentActiveCall.peerConnection.signalingState;
      console.log('[CallContext] ðŸ“Š Current signaling state:', signalingState);
      
      if (signalingState !== 'have-local-offer') {
        console.error(`[CallContext] âŒ Invalid signaling state for answer: ${signalingState}, expected: have-local-offer`);
        return;
      }
      
      await currentActiveCall.peerConnection.setRemoteDescription(new RTCSessionDescription(message.answer));
      console.log('[CallContext] âœ… ENHANCED: Remote description (answer) set successfully on caller side');
      
      // ðŸŽ¯ CRITICAL FIX 5: Start comprehensive transport state monitoring
      console.log('[CallContext] ðŸ”„ ENHANCED: Starting transport connectivity monitoring');
      startTransportStateMonitoring(currentActiveCall.peerConnection, currentActiveCall.callId);
      
      // ðŸŽ¯ CRITICAL FIX 6: Validate transceivers after answer
      console.log('[CallContext] ðŸ“Š Post-answer transceiver validation:');
      const transceivers = currentActiveCall.peerConnection.getTransceivers();
      transceivers.forEach((transceiver, index) => {
        console.log(`[CallContext] Final Transceiver ${index}:`, {
          direction: transceiver.direction,
          currentDirection: transceiver.currentDirection,
          mid: transceiver.mid,
          kind: transceiver.receiver.track?.kind,
          senderTrack: {
            kind: transceiver.sender.track?.kind,
            enabled: transceiver.sender.track?.enabled,
            readyState: transceiver.sender.track?.readyState
          }
        });
        
        // ðŸŽ¯ CRITICAL: Ensure proper sender track is attached
        if (!transceiver.sender.track && transceiver.direction === 'sendrecv') {
          console.warn(`[CallContext] âš ï¸ CRITICAL: No sender track attached for ${transceiver.receiver.track?.kind} transceiver`);
        }
      });
      
      // The ontrack handler should now be triggered for the caller to receive audio
      console.log('[CallContext] ðŸŽ¯ ENHANCED: Waiting for ontrack event to provide remote stream to caller...');
      
    } catch (error) {
      console.error('[CallContext] âŒ Error handling WebRTC answer:', error);
    }
  };

  const handleWebRTCIceCandidate = async (message: any) => {
    console.log('[CallContext] ðŸ§Š ENHANCED: Received ICE candidate for callId:', message.callId);
    
    // Use ref for more stable reference
    const currentActiveCall = activeCallRef.current || activeCall;
    const currentIncomingCall = incomingCallRef.current || incomingCall;
    const currentCall = currentActiveCall || currentIncomingCall;
    
    console.log('[CallContext] Current activeCall for ICE:', currentActiveCall?.callId);
    console.log('[CallContext] Current incomingCall for ICE:', currentIncomingCall?.callId);
    console.log('[CallContext] Looking for callId:', message.callId);
    
    // If no current call matches the callId, queue the candidate for later
    if (!currentCall || !currentCall.peerConnection || currentCall.callId !== message.callId) {
      console.log('[CallContext] No matching call found, queuing ICE candidate for callId:', message.callId);
      console.log('[CallContext] Available calls:', { 
        activeCallId: currentActiveCall?.callId, 
        incomingCallId: currentIncomingCall?.callId 
      });
      pendingIceCandidates.current.push({
        callId: message.callId,
        candidate: message.candidate
      });
      return;
    }

    try {
      // ðŸŽ¯ CRITICAL FIX 7: Enhanced ICE candidate validation
      const candidate = message.candidate;
      console.log('[CallContext] ðŸ§Š ICE candidate details:', {
        type: candidate.candidate?.split(' ')[7] || 'unknown',
        protocol: candidate.protocol,
        address: candidate.address,
        port: candidate.port,
        priority: candidate.priority,
        foundation: candidate.foundation
      });
      
      // Check if remote description is set
      if (!currentCall.peerConnection.remoteDescription) {
        console.log('[CallContext] Remote description not set yet, queuing ICE candidate');
        pendingIceCandidates.current.push({
          callId: message.callId,
          candidate: message.candidate
        });
        return;
      }
      
      await currentCall.peerConnection.addIceCandidate(new RTCIceCandidate(message.candidate));
      console.log('[CallContext] âœ… ENHANCED: Added ICE candidate successfully');
      
      // ðŸŽ¯ CRITICAL FIX 8: Log ICE connection state after candidate addition
      console.log('[CallContext] ðŸ§Š ICE connection state after candidate:', {
        iceConnectionState: currentCall.peerConnection.iceConnectionState,
        iceGatheringState: currentCall.peerConnection.iceGatheringState,
        connectionState: currentCall.peerConnection.connectionState
      });
      
    } catch (error) {
      console.error('[CallContext] âŒ Error handling ICE candidate:', error);
      // Don't fail the call due to one bad ICE candidate
    }
  };

  const startCall = async (peerUserId: number, peerName: string, callType: 'audio' | 'video') => {
    if (!user) {
      console.error('[CallContext] User not available');
      return;
    }

    // Enhanced WebSocket check with simple retry
    if (!ws || ws.readyState !== WebSocket.OPEN) {
      console.log('[CallContext] WebSocket not connected, please refresh page');
      return;
    }

    console.log(`[CallContext] ðŸ“± MOBILE: Starting ${callType} call to:`, peerName, 'from mobile device');
    
    // Check mobile compatibility first
    const { isMobile, isHTTPS, hasMediaDevices } = checkMobileCompatibility();
    
    if (!hasMediaDevices) {
      console.error('[CallContext] Media devices not supported');
      return;
    }

    try {
      // Ultra-simple constraints for maximum mobile compatibility
      let constraints;
      if (callType === 'video') {
        // Try progressively simpler video constraints for mobile
        constraints = { 
          audio: true, 
          video: isMobile ? {
            width: { max: 640 },
            height: { max: 480 },
            facingMode: 'user',
            frameRate: { max: 15 }
          } : {
            width: { ideal: 640 },
            height: { ideal: 480 },
            facingMode: 'user'
          }
        };
      } else {
        constraints = { audio: true };
      }

      console.log('[CallContext] Requesting media permissions with mobile-optimized constraints:', constraints);
      console.log('[CallContext] Device info:', { isMobile, isHTTPS, hasMediaDevices });
      
      // Try to get media stream with better error handling
      const localStream = await navigator.mediaDevices.getUserMedia(constraints).catch(async (error) => {
        console.error('[CallContext] Failed with optimized constraints, trying fallback:', error);
        
        // First fallback - very basic video constraints
        if (callType === 'video') {
          try {
            console.log('[CallContext] ðŸ“± Mobile video fallback 1: basic constraints');
            return await navigator.mediaDevices.getUserMedia({ audio: true, video: true });
          } catch (error2) {
            console.error('[CallContext] ðŸ“± Mobile video fallback 1 failed:', error2);
            
            // Second fallback - try rear camera
            try {
              console.log('[CallContext] ðŸ“± Mobile video fallback 2: rear camera');
              return await navigator.mediaDevices.getUserMedia({ 
                audio: true, 
                video: { facingMode: 'environment' }
              });
            } catch (error3) {
              console.error('[CallContext] ðŸ“± Mobile video fallback 2 failed:', error3);
              
              // Final fallback - audio only for video calls
              console.log('[CallContext] ðŸ“± Mobile video fallback 3: audio only');
              return await navigator.mediaDevices.getUserMedia({ audio: true });
            }
          }
        } else {
          // Audio call fallback
          return await navigator.mediaDevices.getUserMedia({ audio: true });
        }
      });
      console.log('[CallContext] âœ… Successfully got media stream:', {
        audioTracks: localStream.getAudioTracks().length,
        videoTracks: localStream.getVideoTracks().length,
        callType: callType
      });
      
      // Debug track details for video calls
      if (callType === 'video') {
        console.log('[CallContext] ðŸ“¹ Video call tracks details:');
        localStream.getTracks().forEach((track, index) => {
          console.log(`[CallContext] Local track ${index}:`, {
            kind: track.kind,
            enabled: track.enabled,
            readyState: track.readyState,
            id: track.id,
            label: track.label
          });
        });
      }

      const callId = `call_${Date.now()}_${user.id}_${peerUserId}`;

      // Create RTCPeerConnection for intranet/offline mode (no internet required)
      const peerConnection = new RTCPeerConnection({
        iceServers: [], // Empty array for local network only - no internet connection needed
        iceTransportPolicy: 'all', // Allow both UDP and TCP
        bundlePolicy: 'max-bundle',
        rtcpMuxPolicy: 'require'
      });

      // Add local stream to peer connection
      localStream.getTracks().forEach(track => {
        peerConnection.addTrack(track, localStream);
      });

      // Handle incoming remote stream for caller
      peerConnection.ontrack = (event) => {
        console.log('[CallContext] ðŸ“¡ CALLER: Received remote stream in startCall');
        const remoteStream = event.streams[0];
        console.log('[CallContext] ðŸ“¡ CALLER: Remote stream details:', {
          id: remoteStream.id,
          active: remoteStream.active,
          audioTracks: remoteStream.getAudioTracks().length,
          videoTracks: remoteStream.getVideoTracks().length
        });
        
        // Store remote stream globally for AudioCall component
        console.log('[CallContext] ðŸ“¡ CALLER: Storing remote stream globally');
        setRemoteAudioStream(remoteStream);
        
        // Stop waiting tone when remote audio stream is received
        console.log('[CallContext] Remote audio received - stopping waiting tone');
        stopWaitingTone();
        
        // Find and setup audio element for remote stream
        setTimeout(() => {
          const audioElement = document.querySelector('#remoteAudio') as HTMLAudioElement;
          if (audioElement) {
            audioElement.srcObject = remoteStream;
            audioElement.volume = 1.0;
            audioElement.play().then(() => {
              console.log('[CallContext] âœ… CALLER: Remote audio playing successfully in startCall');
            }).catch(e => {
              console.log('[CallContext] CALLER: Remote audio autoplay failed in startCall:', e);
            });
          } else {
            console.log('[CallContext] âŒ CALLER: Remote audio element not found in startCall');
          }
        }, 100);
      };

      // Handle ICE candidates
      peerConnection.onicecandidate = (event) => {
        if (event.candidate && ws && ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify({
            type: 'webrtc_ice_candidate',
            callId: callId,
            candidate: event.candidate
          }));
        }
      };

      const newCall: CallState = {
        callId,
        callType,
        status: 'calling',
        isIncoming: false,
        peerUserId,
        peerName,
        localStream,
        remoteStreams: new Map(),
        peerConnection,
        audioEnabled: true,
        videoEnabled: callType === 'video',
        isMuted: false,
      };

      setActiveCall(newCall);

      // Start waiting tone for outgoing call
      console.log('[CallContext] Starting waiting tone for outgoing call');
      startWaitingTone();

      // Create and send WebRTC offer
      try {
        const offer = await peerConnection.createOffer();
        await peerConnection.setLocalDescription(offer);
        
        // Send offer via WebSocket
        ws.send(JSON.stringify({
          type: 'webrtc_offer',
          callId: callId,
          offer: offer
        }));
      } catch (error) {
        console.error('[CallContext] Error creating WebRTC offer:', error);
      }

      // Send call initiation message (ws is guaranteed to be connected at this point)
      ws!.send(JSON.stringify({
        type: 'initiate_call',
        payload: {
          callId,
          toUserId: peerUserId,
          callType,
          fromUserId: user.id,
          fromUserName: user.callsign,
        }
      }));

      // Navigate to call interface with a small delay to ensure state is set
      setTimeout(() => {
        setLocation(callType === 'video' ? '/video-call' : '/audio-call');
      }, 100);

    } catch (error: any) {
      console.error('[CallContext] Error starting call:', error);
    }
  };

  const acceptCall = async () => {
    console.log('[CallContext] acceptCall function called');
    console.log('[CallContext] incomingCall:', incomingCall);
    console.log('[CallContext] ws connected:', ws?.readyState === WebSocket.OPEN);
    console.log('[CallContext] user:', user);
    
    if (!incomingCall || !ws || !user) {
      console.error('[CallContext] No incoming call or WebSocket not connected');
      return;
    }

    // If it's a group call, use joinGroupCall instead
    if (incomingCall.isGroupCall && incomingCall.callId) {
      console.log('[CallContext] Accepting group call - redirecting to joinGroupCall');
      setIncomingCall(null); // Clear the modal first
      await joinGroupCall(
        incomingCall.callId,
        incomingCall.groupId!,
        incomingCall.groupName!,
        incomingCall.callType
      );
      return;
    }

    console.log('[CallContext] Accepting call');
    
    // IMMEDIATE ringtone stop - use the comprehensive stopAllRingtones function
    console.log('[CallContext] ðŸ”‡ FORCE STOPPING ALL RINGTONES - Call accepted');
    stopAllRingtones();

    try {
      // Enhanced mobile-friendly media constraints for accepting calls
      const constraints = {
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 48000,
          channelCount: 1
        },
        video: incomingCall.callType === 'video' ? {
          facingMode: 'user',
          width: { ideal: 640, max: 1280 },
          height: { ideal: 480, max: 720 },
          frameRate: { ideal: 15, max: 30 }
        } : false
      };

      // Check if mediaDevices is available
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        throw new Error('Media devices not supported on this browser');
      }

      console.log('[CallContext] Requesting media permissions for mobile (accept call)...');
      const localStream = await navigator.mediaDevices.getUserMedia(constraints);
      console.log('[CallContext] Got local media stream for incoming call');

      // Use existing peerConnection from incomingCall (already setup)
      const peerConnection = incomingCall.peerConnection;
      if (!peerConnection) {
        throw new Error('No peerConnection found in incoming call');
      }

      // Add local stream to existing peer connection
      localStream.getTracks().forEach(track => {
        peerConnection.addTrack(track, localStream);
      });

      console.log('[CallContext] Added local stream to existing peerConnection');

      // Setup remote stream handler for receiver
      peerConnection.ontrack = (event) => {
        console.log('[CallContext] ðŸ“¡ RECEIVER: Received remote stream');
        const remoteStream = event.streams[0];
        console.log('[CallContext] ðŸ“¡ RECEIVER: Remote stream details:', {
          id: remoteStream.id,
          active: remoteStream.active,
          audioTracks: remoteStream.getAudioTracks().length,
          videoTracks: remoteStream.getVideoTracks().length
        });
        
        // Store remote stream globally for AudioCall component
        console.log('[CallContext] ðŸ“¡ RECEIVER: Storing remote stream globally');
        setRemoteAudioStream(remoteStream);
        
        // Stop waiting tone when remote audio stream is received
        console.log('[CallContext] Remote audio received - stopping waiting tone');
        stopWaitingTone();
        
        // Find and setup audio element for remote stream
        setTimeout(() => {
          const audioElement = document.querySelector('#remoteAudio') as HTMLAudioElement;
          if (audioElement) {
            audioElement.srcObject = remoteStream;
            audioElement.volume = 1.0;
            audioElement.play().then(() => {
              console.log('[CallContext] âœ… RECEIVER: Remote audio playing successfully');
            }).catch(e => {
              console.log('[CallContext] RECEIVER: Remote audio autoplay failed:', e);
            });
          } else {
            console.log('[CallContext] âŒ RECEIVER: Remote audio element not found');
          }
        }, 100);
      };

      // ðŸŽ¯ CRITICAL: Now that call is accepted, enable audio and activate pending remote stream
      console.log('[CallContext] ðŸŽ¯ Call accepted - enabling audio and activating remote stream');
      
      // Check if there's a pending remote stream from earlier ontrack event
      if ((window as any).__pendingRemoteStream) {
        const pendingStream = (window as any).__pendingRemoteStream;
        console.log('[CallContext] ðŸŽ¯ Activating pending remote stream with tracks:', pendingStream.getTracks().length);
        
        // Enable all audio tracks now that call is accepted
        pendingStream.getAudioTracks().forEach(track => {
          track.enabled = true;
          console.log('[CallContext] ðŸ”Š Audio track enabled after call accepted');
        });
        
        // Now it's safe to store the remote stream
        setRemoteAudioStream(pendingStream);
        
        // Clear pending stream
        (window as any).__pendingRemoteStream = null;
      }

      // Accept the call
      setActiveCall({
        ...incomingCall,
        status: 'connected',
        localStream,
        startTime: new Date(),
        audioEnabled: true, // âœ… Audio enabled AFTER call accepted
        videoEnabled: incomingCall.callType === 'video', // âœ… Video enabled AFTER call accepted
        isMuted: false, // âœ… Unmuted AFTER call accepted
      });

      setIncomingCall(null);

      // Send call acceptance message
      ws.send(JSON.stringify({
        type: 'accept_call',
        payload: {
          callId: incomingCall.callId,
          toUserId: incomingCall.peerUserId,
          fromUserId: user.id,
        }
      }));

      // If it's a group call, send join notification
      if (incomingCall.isGroupCall) {
        console.log('[CallContext] Joining group call:', incomingCall.callId);
        setTimeout(() => {
          ws.send(JSON.stringify({
            type: 'join_group_call',
            payload: {
              callId: incomingCall.callId,
              groupId: incomingCall.groupId,
              fromUserId: user.id
            }
          }));
          console.log('[CallContext] Sent join group call message for accepted call');
        }, 200);
      }

      // Store callId and peerUserId before clearing incomingCall
      const callId = incomingCall.callId;
      const peerUserId = incomingCall.peerUserId;

      // Send ready signal after a short delay to ensure everything is set up
      setTimeout(() => {
        if (ws && ws.readyState === WebSocket.OPEN) {
          console.log('[CallContext] ðŸ“¤ Sending WebRTC ready signal to caller:', peerUserId);
          ws.send(JSON.stringify({
            type: 'webrtc_ready',
            payload: {
              callId: callId,
              toUserId: peerUserId,
            }
          }));
          console.log('[CallContext] âœ… WebRTC ready signal sent successfully');
        } else {
          console.error('[CallContext] âŒ Cannot send WebRTC ready - WebSocket not connected');
        }
      }, 200);

      // Store callType before clearing incomingCall
      const callType = incomingCall.callType;
      
      // Navigate to call interface with a small delay to ensure state is set
      setTimeout(() => {
        setLocation(callType === 'video' ? '/video-call' : '/audio-call');
      }, 100);

    } catch (error: any) {
      console.error('[CallContext] Error accepting call:', error);
    }
  };

  const rejectCall = () => {
    if (!incomingCall || !ws || !user) {
      console.error('[CallContext] No incoming call or WebSocket not connected');
      return;
    }

    console.log('[CallContext] ðŸš« Rejecting call:', incomingCall.callId);
    console.log('[CallContext] ðŸš« Call details:', {
      isGroupCall: incomingCall.isGroupCall,
      groupName: incomingCall.groupName,
      callType: incomingCall.callType,
      fromUser: incomingCall.peerName
    });
    
    // FORCE STOP ALL WAITING TONES AND RINGTONES IMMEDIATELY
    console.log('[CallContext] FORCE STOPPING ALL RINGTONES - reject call');
    stopAllRingtones();
    stopWaitingTone();
    
    // Additional comprehensive audio cleanup for reject
    if (waitingToneInterval) {
      clearInterval(waitingToneInterval);
      setWaitingToneInterval(null);
      console.log('[CallContext] âœ… Waiting tone interval cleared on reject');
    }
    
    // Stop any running audio contexts or oscillators
    if (audioContext) {
      try {
        audioContext.close();
        setAudioContext(null);
        console.log('[CallContext] âœ… Audio context closed on reject');
      } catch (e) {
        console.log('[CallContext] Audio context already closed');
      }
    }

    // Stop ringtone when call is rejected
    if (ringtoneAudio) {
      ringtoneAudio.pause();
      ringtoneAudio.currentTime = 0;
    }

    // Handle group call rejection vs individual call rejection
    if (incomingCall.isGroupCall) {
      console.log('[CallContext] ðŸš« Rejecting group call - sending reject_group_call message');
      
      // Send group call rejection message
      ws.send(JSON.stringify({
        type: 'reject_group_call',
        payload: {
          callId: incomingCall.callId,
          groupId: incomingCall.groupId,
          fromUserId: user.id,
          userName: user.callsign
        }
      }));
      
      // Clean up any pre-created peer connection for group call
      if (incomingCall.peerConnection) {
        console.log('[CallContext] ðŸ§¹ Cleaning up group call peer connection');
        try {
          incomingCall.peerConnection.close();
        } catch (e) {
          console.warn('[CallContext] Error closing peer connection:', e);
        }
      }
      
    } else {
      console.log('[CallContext] ðŸš« Rejecting individual call - sending reject_call message');
      
      // Send individual call rejection message
      ws.send(JSON.stringify({
        type: 'reject_call',
        payload: {
          callId: incomingCall.callId,
          toUserId: incomingCall.peerUserId,
          fromUserId: user.id,
        }
      }));
    }

    // Clear any pending remote stream to prevent early media leak
    if ((window as any).__pendingRemoteStream) {
      const pendingStream = (window as any).__pendingRemoteStream;
      console.log('[CallContext] ðŸ›¡ï¸ Clearing pending remote stream on reject to prevent early media');
      
      // Stop all tracks in pending stream
      pendingStream.getTracks().forEach(track => {
        track.stop();
        console.log('[CallContext] ðŸ›‘ Stopped pending track on reject:', track.kind);
      });
      
      // Clear pending stream
      (window as any).__pendingRemoteStream = null;
    }

    // Clear the incoming call state
    setIncomingCall(null);
    console.log('[CallContext] âœ… Incoming call cleared after rejection');
  };

  const hangupCall = () => {
    console.log('[CallContext] Hanging up call - start');
    
    // FORCE STOP ALL WAITING TONES AND RINGTONES IMMEDIATELY
    console.log('[CallContext] FORCE STOPPING ALL RINGTONES - hangup call');
    stopAllRingtones();
    stopWaitingTone();
    
    // Additional comprehensive audio cleanup for hangup
    if (waitingToneInterval) {
      clearInterval(waitingToneInterval);
      setWaitingToneInterval(null);
      console.log('[CallContext] âœ… Waiting tone interval cleared on hangup');
    }
    
    // Clear stored interval ID for waiting tone
    if ((window as any).__waitingToneIntervalId) {
      console.log('[CallContext] Clearing stored waiting tone interval on hangup');
      clearInterval((window as any).__waitingToneIntervalId);
      (window as any).__waitingToneIntervalId = null;
    }
    
    // Stop any running audio contexts or oscillators
    if (audioContext) {
      try {
        audioContext.close();
        setAudioContext(null);
        console.log('[CallContext] âœ… Audio context closed on hangup');
      } catch (e) {
        console.log('[CallContext] Audio context already closed');
      }
    }
    
    // Force stop all audio elements on the page
    try {
      const audioElements = document.querySelectorAll('audio');
      audioElements.forEach((audio: HTMLAudioElement) => {
        audio.pause();
        audio.currentTime = 0;
        audio.volume = 0;
        audio.muted = true;
      });
      console.log('[CallContext] âœ… All page audio elements forcefully stopped on hangup');
    } catch (e) {
      console.log('[CallContext] Error stopping all audio elements:', e);
    }
    
    // Stop ringtone when call is ended
    if (ringtoneAudio) {
      ringtoneAudio.pause();
      ringtoneAudio.currentTime = 0;
    }
    
    try {
      // Stop local media stream safely
      if (activeCall?.localStream) {
        console.log('[CallContext] Stopping local media tracks');
        activeCall.localStream.getTracks().forEach(track => {
          try {
            track.stop();
            console.log('[CallContext] Stopped local track:', track.kind);
          } catch (err) {
            console.warn('[CallContext] Error stopping local track:', err);
          }
        });
      }

      // Clean up all remote audio elements for group calls
      if (activeCall?.isGroupCall && activeCall?.participants) {
        console.log('[CallContext] Cleaning up group call audio elements');
        activeCall.participants.forEach(participant => {
          const audioElement = document.getElementById(`groupAudio-${participant.userId}`);
          if (audioElement) {
            try {
              const audioEl = audioElement as HTMLAudioElement;
              // Stop the audio stream
              if (audioEl.srcObject) {
                const stream = audioEl.srcObject as MediaStream;
                stream.getTracks().forEach(track => {
                  track.stop();
                  console.log('[CallContext] Stopped remote audio track for participant:', participant.userId);
                });
                audioEl.srcObject = null;
              }
              // Remove the audio element
              audioElement.remove();
              console.log('[CallContext] Removed audio element for participant:', participant.userId);
            } catch (err) {
              console.warn('[CallContext] Error cleaning up audio for participant:', participant.userId, err);
            }
          }
        });
      }

      // Clean up any remaining group audio elements
      const allGroupAudioElements = document.querySelectorAll('[id^="groupAudio-"]');
      allGroupAudioElements.forEach(element => {
        try {
          const audioEl = element as HTMLAudioElement;
          if (audioEl.srcObject) {
            const stream = audioEl.srcObject as MediaStream;
            stream.getTracks().forEach(track => track.stop());
            audioEl.srcObject = null;
          }
          element.remove();
          console.log('[CallContext] Cleaned up remaining group audio element');
        } catch (err) {
          console.warn('[CallContext] Error cleaning up remaining audio element:', err);
        }
      });

      // Clean up any remaining group video elements
      const allGroupVideoElements = document.querySelectorAll('[id^="groupVideo-"]');
      allGroupVideoElements.forEach(element => {
        try {
          const videoEl = element as HTMLVideoElement;
          if (videoEl.srcObject) {
            const stream = videoEl.srcObject as MediaStream;
            stream.getTracks().forEach(track => {
              track.stop();
              console.log('[CallContext] Stopped video track:', track.kind);
            });
            videoEl.srcObject = null;
          }
          element.remove();
          console.log('[CallContext] Cleaned up remaining group video element');
        } catch (err) {
          console.warn('[CallContext] Error cleaning up remaining video element:', err);
        }
      });

      // Force cleanup of all video elements that might still have active streams
      const allVideoElements = document.querySelectorAll('video');
      allVideoElements.forEach(videoEl => {
        try {
          if (videoEl.srcObject) {
            const stream = videoEl.srcObject as MediaStream;
            stream.getTracks().forEach(track => {
              if (track.readyState !== 'ended') {
                track.stop();
                console.log('[CallContext] Force stopped remaining video track:', track.kind);
              }
            });
            videoEl.srcObject = null;
            videoEl.load(); // Force reload
          }
        } catch (err) {
          console.warn('[CallContext] Error force cleaning video element:', err);
        }
      });

      // Send call end message safely
      if (ws && ws.readyState === WebSocket.OPEN && activeCall && user) {
        try {
          // Check if it's a group call
          if (activeCall.isGroupCall) {
            console.log('[CallContext] Ending group call:', activeCall.callId);
            ws.send(JSON.stringify({
              type: 'end_call',
              payload: {
                callId: activeCall.callId,
                fromUserId: user.id,
                isGroupCall: true,
                groupId: activeCall.groupId
              }
            }));
          } else {
            // Regular 1-on-1 call
            ws.send(JSON.stringify({
              type: 'end_call',
              payload: {
                callId: activeCall.callId,
                toUserId: activeCall.peerUserId,
                fromUserId: user.id,
              }
            }));
          }
        } catch (err) {
          console.warn('[CallContext] Error sending end call message:', err);
        }
      }

      // Clear call state and navigate back to chat
      setActiveCall(null);
      setRemoteAudioStream(null);
      setIncomingCall(null);
      
      // Additional cleanup - request browser to release all media devices
      setTimeout(() => {
        try {
          // Force stop all active media tracks by accessing global MediaStreamTrack objects
          if (window.navigator?.mediaDevices) {
            console.log('[CallContext] Requesting media device cleanup');
          }
        } catch (err) {
          console.warn('[CallContext] Media device cleanup warning:', err);
        }
      }, 200);
      
      setLocation('/chat');
      
      console.log('[CallContext] Call ended successfully, all audio streams stopped, navigated back to chat');
      
    } catch (error) {
      console.error('[CallContext] Error during hangup:', error);
      // Still clear the call state even if there's an error
      setActiveCall(null);
      setLocation('/chat');
    }
  };

  const toggleCallAudio = () => {
    if (!activeCall?.localStream) return;

    const audioTrack = activeCall.localStream.getAudioTracks()[0];
    if (audioTrack) {
      audioTrack.enabled = !audioTrack.enabled;
      setActiveCall({
        ...activeCall,
        audioEnabled: audioTrack.enabled,
      });
    }
  };

  const toggleCallVideo = () => {
    if (!activeCall?.localStream) return;

    const videoTrack = activeCall.localStream.getVideoTracks()[0];
    if (videoTrack) {
      videoTrack.enabled = !videoTrack.enabled;
      setActiveCall({
        ...activeCall,
        videoEnabled: videoTrack.enabled,
      });
    }
  };

  const toggleMute = () => {
    if (!activeCall?.localStream) return;

    const audioTrack = activeCall.localStream.getAudioTracks()[0];
    if (audioTrack) {
      audioTrack.enabled = !activeCall.isMuted;
      setActiveCall({
        ...activeCall,
        isMuted: !activeCall.isMuted,
      });
    }
  };

  const switchCallCamera = async () => {
    console.log('[CallContext] ðŸ”¥ SWITCH CAMERA FUNCTION CALLED!');
    console.log('[CallContext] Current activeCall:', activeCall);
    console.log('[CallContext] Has localStream:', !!activeCall?.localStream);
    console.log('[CallContext] Call type:', activeCall?.callType);
    
    if (!activeCall?.localStream || activeCall.callType !== 'video') {
      console.log('[CallContext] No active video call or stream available');
      return;
    }

    try {
      console.log('[CallContext] Starting camera switch...');
      
      // Mobile device detection and debugging
      const isMobileDevice = /Android|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
      const debugInfo = `Device: ${isMobileDevice ? 'Mobile' : 'Desktop'}\nUser Agent: ${navigator.userAgent.slice(0, 80)}...`;
      console.log('[CallContext] Device detection:', debugInfo);
      
      if (isMobileDevice) {
        console.log('[CallContext] Mobile device detected, checking permissions...');
        try {
          const permissions = await navigator.permissions.query({ name: 'camera' as PermissionName });
          console.log('[CallContext] Camera permission state:', permissions.state);
          if (permissions.state === 'denied') {
            console.log('[CallContext] Camera permission denied');
            return;
          }
        } catch (permError) {
          console.log('[CallContext] Permission check not supported, continuing...');
        }
      }
      
      // Get current video track
      const currentVideoTrack = activeCall.localStream.getVideoTracks()[0];
      if (!currentVideoTrack) {
        console.log('[CallContext] No video track found in current stream');
        return;
      }

      // Get current settings to determine which camera is active
      const currentSettings = currentVideoTrack.getSettings();
      const currentFacingMode = currentSettings.facingMode;
      console.log('[CallContext] Current camera facingMode:', currentFacingMode);

      // Enumerate available video devices with mobile debugging
      const devices = await navigator.mediaDevices.enumerateDevices();
      const videoDevices = devices.filter(device => device.kind === 'videoinput');
      console.log('[CallContext] Available video devices:', videoDevices.length);
      
      // Log camera info for debugging
      console.log(`[CallContext] Mobile device camera detection: ${videoDevices.length} cameras found`);
      
      videoDevices.forEach((device, index) => {
        console.log(`[CallContext] Camera ${index + 1}:`, {
          label: device.label,
          deviceId: device.deviceId.slice(0, 8),
          groupId: device.groupId?.slice(0, 8)
        });
      });

      // Check camera count for switching strategy
      if (videoDevices.length <= 1) {
        console.log('[CallContext] Only one camera detected');
        
        if (isMobileDevice) {
          console.log('[CallContext] Mobile device detected - attempting force switch');
          // On mobile, try to switch even with 1 detected camera
          // Some mobile browsers don't properly enumerate all cameras
        } else {
          console.log('[CallContext] Desktop device - cannot switch with only 1 camera');
          return;
        }
      } else {
        console.log(`[CallContext] ${videoDevices.length} cameras detected - normal switch`);
      }

      // NEW STRATEGY: For HP with 4 cameras, specifically target back camera
      let nextFacingMode: string = 'user';
      let targetDeviceId: string | undefined;
      let rearCameras: MediaDeviceInfo[] = [];
      let frontCameras: MediaDeviceInfo[] = [];
      
      if (videoDevices.length > 1) {
        // Multiple cameras detected - ENHANCED STRATEGY for 4-camera phones
        const currentDeviceId = currentSettings.deviceId;
        console.log('[CallContext] Current device ID:', currentDeviceId?.slice(0, 8));
        
        // ENHANCED: Find ALL possible rear cameras - be MORE SELECTIVE
        rearCameras = videoDevices.filter(device => {
          const label = device.label.toLowerCase();
          return (
            label.includes('back') || 
            label.includes('rear') || 
            label.includes('environment') ||
            (label.includes('camera') && (label.includes('2') || label.includes('0'))) ||
            // Exclude definite front cameras
            (!label.includes('front') && !label.includes('user') && !label.includes('selfie') && !label.includes('1'))
          );
        });
        
        // Find front cameras
        frontCameras = videoDevices.filter(device => 
          device.label.toLowerCase().includes('front') || 
          device.label.toLowerCase().includes('user') ||
          device.label.toLowerCase().includes('selfie') ||
          device.label.toLowerCase().includes('1') // Sometimes front is camera1
        );
        
        // Log camera analysis for debugging (console only)
        console.log('[CallContext] Camera Analysis:');
        console.log('Rear cameras:', rearCameras.map(c => c.label));
        console.log('Front cameras:', frontCameras.map(c => c.label));
        
        // Choose primary cameras
        const rearCamera = rearCameras[0]; // Take first rear camera
        const frontCamera = frontCameras[0]; // Take first front camera
        
        // Determine if current camera is front or back
        const isCurrentlyFront = currentFacingMode === 'user' || 
                                currentSettings.deviceId === frontCamera?.deviceId;
        
        if (isCurrentlyFront && rearCamera) {
          // Switch TO rear camera
          nextFacingMode = 'environment';
          targetDeviceId = rearCamera.deviceId;
          console.log('[CallContext] Switching TO rear camera:', rearCamera.label, 'ID:', targetDeviceId?.slice(0, 8));
        } else if (!isCurrentlyFront && frontCamera) {
          // Switch TO front camera
          nextFacingMode = 'user';
          targetDeviceId = frontCamera.deviceId;
          console.log('[CallContext] Switching TO front camera:', frontCamera.label, 'ID:', targetDeviceId?.slice(0, 8));
        } else {
          // Fallback: cycle through devices
          const currentIndex = videoDevices.findIndex(device => device.deviceId === currentDeviceId);
          const nextIndex = (currentIndex + 1) % videoDevices.length;
          const nextDevice = videoDevices[nextIndex];
          nextFacingMode = currentFacingMode === 'user' ? 'environment' : 'user';
          targetDeviceId = nextDevice.deviceId;
          console.log('[CallContext] Fallback cycling to device:', nextDevice.label);
        }
      } else {
        // Single camera or mobile force switch - toggle facing mode
        nextFacingMode = currentFacingMode === 'user' ? 'environment' : 'user';
        console.log('[CallContext] Force switching from facingMode:', currentFacingMode, 'to:', nextFacingMode);
        
        if (isMobileDevice && nextFacingMode === 'environment') {
          console.log('[CallContext] ðŸ“± Attempting to access rear camera on mobile device');
        }
        
        // For single camera devices, still try to populate camera arrays
        if (nextFacingMode === 'environment') {
          rearCameras = videoDevices; // Assume current device might be rear
        } else {
          frontCameras = videoDevices; // Assume current device might be front
        }
      }

      // For mobile devices with rear camera attempt, use more specific constraints
      const isMobileRearCamera = isMobileDevice && nextFacingMode === 'environment';
      console.log('[CallContext] Is mobile rear camera attempt:', isMobileRearCamera);

      // Get new video stream with 4-tier fallback system (like CameraTestSimple)
      let newVideoStream: MediaStream;
      let newVideoTrack: MediaStreamTrack;
      
      // SIMPLIFIED STRATEGY: Focus on basic rear camera access
      const strategies = [];
      
      if (isMobileRearCamera) {
        console.log('[CallContext] Attempting rear camera access with simplified strategies');
        
        // Strategy 1: Just exact environment - paling basic
        strategies.push({
          video: { facingMode: { exact: 'environment' } }
        });
        
        // Strategy 2: Basic environment tanpa exact
        strategies.push({
          video: { facingMode: 'environment' }
        });
        
        // Strategy 3: Try specific device IDs jika ada
        if (rearCameras && rearCameras.length > 0) {
          rearCameras.forEach((camera) => {
            strategies.push({
              video: { deviceId: { exact: camera.deviceId } }
            });
          });
        }
        
        // Strategy 4: Force rear with minimal constraints
        strategies.push({
          video: { 
            facingMode: { exact: 'environment' },
            width: { ideal: 640 },
            height: { ideal: 480 }
          }
        });
        
      } else {
        // Front camera strategies (keep simple)
        if (targetDeviceId) {
          strategies.push({
            video: { deviceId: { exact: targetDeviceId } }
          });
        }
        
        strategies.push({
          video: { facingMode: { exact: 'user' } }
        });
        
        strategies.push({
          video: { facingMode: 'user' }
        });
      }
      
      // Always add fallback
      strategies.push({
        video: true
      });

      let lastError: Error | null = null;
      let strategySucceeded = false;

      for (const [index, strategy] of strategies.entries()) {
        try {
          console.log(`[CallContext] Trying camera switch strategy ${index + 1} for ${isMobileRearCamera ? 'rear' : 'front'} camera:`, strategy);
          
          newVideoStream = await navigator.mediaDevices.getUserMedia(strategy);
          newVideoTrack = newVideoStream.getVideoTracks()[0];
          console.log(`[CallContext] Strategy ${index + 1} SUCCESS! Got video track:`, newVideoTrack.id);
          
          // Log success for debugging
          const newSettings = newVideoTrack.getSettings();
          const cameraType = newSettings.facingMode === 'environment' ? 'rear' : 
                            newSettings.facingMode === 'user' ? 'front' : 'unknown';
          console.log(`[CallContext] Camera switch successful - Type: ${cameraType}, Resolution: ${newSettings.width}x${newSettings.height}`);
          
          strategySucceeded = true;
          break;
        } catch (err) {
          console.log(`[CallContext] Strategy ${index + 1} failed:`, err);
          lastError = err as Error;
          
          // Debug: show which strategy failed for mobile users
          if (isMobileDevice && isMobileRearCamera) {
            const errorName = (err as Error).name || 'Unknown';
            console.log(`[CallContext] Mobile strategy ${index + 1} error: ${errorName}`);
          }
        }
      }

      if (!strategySucceeded || !newVideoStream) {
        // Log failure for debugging and show simple error message
        console.error('[CallContext] Camera switch failed:', lastError);
        if (isMobileDevice && isMobileRearCamera) {
          // Only show simple message for rear camera failures on mobile
          console.log('[CallContext] Rear camera access blocked - falling back to front camera');
        }
        throw lastError || new Error('Semua strategi camera switch gagal');
      }

      // Replace video track in peer connection with safety checks
      if (activeCall.peerConnection && typeof activeCall.peerConnection.getSenders === 'function') {
        try {
          const senders = activeCall.peerConnection.getSenders();
          const videoSender = senders.find(s => s.track && s.track.kind === 'video');
          if (videoSender && typeof videoSender.replaceTrack === 'function') {
            await videoSender.replaceTrack(newVideoTrack);
            console.log('[CallContext] Video track replaced in peer connection');
          } else {
            console.log('[CallContext] No video sender found in peer connection');
          }
        } catch (error) {
          console.error('[CallContext] Error replacing track in peer connection:', error);
        }
      } else {
        console.log('[CallContext] No valid peer connection available for track replacement');
      }

      // Remove old track from local stream and add new one
      activeCall.localStream.removeTrack(currentVideoTrack);
      activeCall.localStream.addTrack(newVideoTrack);
      
      // Stop the old track
      currentVideoTrack.stop();
      
      // Update activeCall state with new stream
      setActiveCall(prev => prev ? {
        ...prev,
        localStream: activeCall.localStream
      } : null);

      console.log('[CallContext] Camera switch completed successfully');

    } catch (error) {
      console.error('[CallContext] Error switching camera:', error);
      
      // Show user-friendly error message with mobile-specific tips
      let errorMessage = 'Gagal mengganti kamera: ';
      if (error instanceof Error && error.name === 'NotAllowedError') {
        errorMessage += 'Izin kamera diperlukan. Buka Pengaturan browser â†’ Izin situs â†’ Kamera â†’ Izinkan.';
      } else if (error instanceof Error && error.name === 'NotFoundError') {
        errorMessage += nextFacingMode === 'environment' 
          ? 'Kamera belakang tidak ditemukan. HP ini mungkin hanya memiliki kamera depan.'
          : 'Kamera depan tidak ditemukan.';
      } else if (error instanceof Error && error.name === 'NotReadableError') {
        errorMessage += 'Kamera sedang digunakan aplikasi lain. Tutup aplikasi kamera lain dan coba lagi.';
      } else if (error instanceof Error && error.name === 'OverconstrainedError') {
        errorMessage += 'Kamera tidak mendukung resolusi yang diminta. Coba restart browser.';
      } else {
        errorMessage += error instanceof Error ? error.message : 'Error tidak diketahui. Coba restart browser atau HP.';
      }
      
      console.log('[CallContext] Camera switch error details:', error);
      alert(errorMessage);
    }
  };

  // Start group call function
  const startGroupCall = async (groupId: number, groupName: string, callType: 'audio' | 'video') => {
    console.log('[CallContext] Starting group call:', { groupId, groupName, callType });
    
    if (!user || !ws) {
      console.error('[CallContext] Cannot start group call - user not authenticated or WebSocket not connected');
      alert('Tidak dapat memulai panggilan grup. Pastikan Anda terhubung ke server.');
      return;
    }

    try {
      // Check mobile compatibility
      const { isMobile, isHTTPS, hasMediaDevices } = checkMobileCompatibility();
      console.log('[CallContext] Mobile compatibility check for group call:', { isMobile, isHTTPS, hasMediaDevices });

      if (!hasMediaDevices) {
        throw new Error('Media devices not supported');
      }

      // Full audio and video enabled from start for group calls
      const constraints = {
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 48000,
          channelCount: 1
        },
        video: {
          width: { ideal: 640, max: 1280 },
          height: { ideal: 480, max: 720 },
          frameRate: { ideal: 24, max: 30 },
          facingMode: 'user' // Front camera for group calls
        }
      };

      console.log('[CallContext] Requesting full media permissions for group call (audio + video enabled)...');
      const localStream = await navigator.mediaDevices.getUserMedia(constraints);
      console.log('[CallContext] Got audio and video stream for group call - full media enabled from start');

      const callId = `group_call_${Date.now()}_${groupId}_${user.id}`;

      // Enhanced RTCPeerConnection for group calls with better stability
      const peerConnection = new RTCPeerConnection({
        iceServers: [], // Empty array for local intranet operation
        iceTransportPolicy: 'all',
        bundlePolicy: 'max-bundle',
        rtcpMuxPolicy: 'require',
        iceCandidatePoolSize: 10 // Pre-gather candidates for faster connection
      });

      // Enhanced connection state monitoring for group calls
      peerConnection.onconnectionstatechange = () => {
        console.log('[CallContext] Group call connection state:', peerConnection.connectionState);
        
        if (peerConnection.connectionState === 'connected') {
          console.log('[CallContext] âœ… Group call WebRTC connection established successfully');
          setActiveCall(prev => prev ? { ...prev, status: 'connected' } : null);
        } else if (peerConnection.connectionState === 'failed' || peerConnection.connectionState === 'disconnected') {
          console.log('[CallContext] âŒ Group call connection failed/disconnected - attempting recovery');
          
          // Auto-recovery for failed connections
          setTimeout(() => {
            if (peerConnection.connectionState === 'failed') {
              console.log('[CallContext] Restarting ICE for group call recovery...');
              peerConnection.restartIce();
            }
          }, 2000);
        }
      };

      // Enhanced ICE connection state monitoring with timeout
      peerConnection.oniceconnectionstatechange = () => {
        console.log('[CallContext] Group call ICE connection state:', peerConnection.iceConnectionState);
        
        if (peerConnection.iceConnectionState === 'checking') {
          setActiveCall(prev => prev ? { ...prev, status: 'calling' } : null);
          
          // Set timeout for checking state - auto-recovery if stuck
          setTimeout(() => {
            if (peerConnection.iceConnectionState === 'checking') {
              console.log('[CallContext] ICE checking timeout - attempting restart');
              peerConnection.restartIce();
            }
          }, 15000); // 15 second timeout
          
        } else if (peerConnection.iceConnectionState === 'connected' || peerConnection.iceConnectionState === 'completed') {
          console.log('[CallContext] âœ… Group call ICE connection established successfully');
          setActiveCall(prev => prev ? { ...prev, status: 'connected' } : null);
        } else if (peerConnection.iceConnectionState === 'failed') {
          console.log('[CallContext] âŒ ICE connection failed - attempting immediate recovery');
          setTimeout(() => {
            console.log('[CallContext] Executing ICE restart for failed connection');
            peerConnection.restartIce();
          }, 1000);
        } else if (peerConnection.iceConnectionState === 'disconnected') {
          console.log('[CallContext] ICE disconnected - monitoring for reconnection');
          setTimeout(() => {
            if (peerConnection.iceConnectionState === 'disconnected') {
              console.log('[CallContext] ICE still disconnected - attempting restart');
              peerConnection.restartIce();
            }
          }, 5000);
        }
      };

      // Enhanced ICE candidate gathering with timeout
      peerConnection.onicegatheringstatechange = () => {
        console.log('[CallContext] ICE gathering state:', peerConnection.iceGatheringState);
        
        if (peerConnection.iceGatheringState === 'complete') {
          console.log('[CallContext] âœ… ICE candidate gathering completed for group call');
        }
      };

      // Add ontrack event for receiving remote streams
      peerConnection.ontrack = (event) => {
        console.log('[CallContext] âœ… Received remote track for group call:', event.track.kind);
        const [remoteStream] = event.streams;
        
        if (remoteStream) {
          console.log('[CallContext] Remote stream details:', {
            id: remoteStream.id,
            active: remoteStream.active,
            audioTracks: remoteStream.getAudioTracks().length,
            videoTracks: remoteStream.getVideoTracks().length
          });
          
          setActiveCall(prev => {
            if (prev?.isGroupCall) {
              const newRemoteStreams = new Map(prev.remoteStreams);
              // Use stream ID as key for better tracking
              newRemoteStreams.set(remoteStream.id, remoteStream);
              
              console.log('[CallContext] Updated remote streams:', Array.from(newRemoteStreams.keys()));
              return { ...prev, remoteStreams: newRemoteStreams };
            }
            return prev;
          });
        }
      };

      // Add local stream to peer connection
      localStream.getTracks().forEach(track => {
        peerConnection.addTrack(track, localStream);
        console.log('[CallContext] Added local track to group call:', track.kind);
      });

      // Set up group call state
      const groupCallState: CallState = {
        callId,
        callType,
        status: 'calling',
        isIncoming: false,
        localStream,
        remoteStreams: new Map(),
        peerConnection,
        audioEnabled: true,
        videoEnabled: callType === 'video',
        isMuted: false,
        startTime: new Date(),
        isGroupCall: true,
        groupId,
        groupName,
        participants: []
      };

      setActiveCall(groupCallState);

      // Send group call invitation to server
      ws.send(JSON.stringify({
        type: 'start_group_call',
        payload: {
          callId,
          groupId,
          groupName,
          callType,
          fromUserId: user.id,
          fromUserName: user.callsign || user.fullName || 'Unknown'
        }
      }));

      console.log('[CallContext] Group call invitation sent');
      
      // Auto-join the group call as the initiator
      setTimeout(() => {
        const joinMessage = {
          type: 'join_group_call',
          payload: {
            callId,
            groupId,
            fromUserId: user.id
          }
        };
        
        ws.send(JSON.stringify(joinMessage));
        console.log('[CallContext] Auto-joined group call as initiator:', joinMessage);
      }, 500); // Small delay to ensure call is created first

      // Note: Participant detection now works through automatic group_call_participants_update
      // No need for manual request_group_participants messages

      // Navigate to appropriate group call interface based on call type
      setTimeout(() => {
        if (callType === 'video') {
          setLocation('/group-video-call');
        } else {
          setLocation('/group-call');
        }
      }, 100);

    } catch (error: any) {
      console.error('[CallContext] Error starting group call:', error);
      alert(getMobileErrorMessage(error).replace('Gagal memulai panggilan', 'Gagal memulai panggilan grup'));
    }
  };

  // Join an existing group call
  const joinGroupCall = async (callId: string, groupId: number, groupName: string, callType: 'audio' | 'video') => {
    try {
      console.log('[CallContext] Joining existing group call:', { callId, groupId, groupName, callType });
      
      if (!user?.id || !ws) {
        throw new Error('User not authenticated or WebSocket not connected');
      }

      // Try to reuse existing stream from incoming call if available, otherwise create new one
      let localStream: MediaStream;
      
      if (incomingCall?.localStream) {
        console.log('[CallContext] Reusing existing stream from incoming call - preserving video state');
        localStream = incomingCall.localStream;
        
        // Ensure video track is enabled if it exists
        const videoTrack = localStream.getVideoTracks()[0];
        if (videoTrack) {
          videoTrack.enabled = true;
          console.log('[CallContext] âœ… Video track enabled from existing stream for group call');
        }
      } else {
        // Create new stream with full media enabled
        const constraints = {
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
            sampleRate: 48000,
            channelCount: 1
          },
          video: {
            width: { ideal: 640, max: 1280 },
            height: { ideal: 480, max: 720 },
            frameRate: { ideal: 24, max: 30 },
            facingMode: 'user' // Front camera for group calls
          }
        };

        console.log('[CallContext] Creating new stream for joining group call (audio + video enabled)...');
        localStream = await navigator.mediaDevices.getUserMedia(constraints);
        console.log('[CallContext] Got new audio and video stream for joining group call - full media enabled from start');
      }

      // Enhanced RTCPeerConnection for joining group calls
      const peerConnection = new RTCPeerConnection({
        iceServers: [], // Empty array for local intranet operation
        iceTransportPolicy: 'all',
        bundlePolicy: 'max-bundle',
        rtcpMuxPolicy: 'require',
        iceCandidatePoolSize: 10
      });

      // Connection monitoring for joining group call
      peerConnection.onconnectionstatechange = () => {
        console.log('[CallContext] Join group call connection state:', peerConnection.connectionState);
        
        if (peerConnection.connectionState === 'connected') {
          console.log('[CallContext] âœ… Successfully joined group call');
          setActiveCall(prev => prev ? { ...prev, status: 'connected' } : null);
        } else if (peerConnection.connectionState === 'failed') {
          console.log('[CallContext] âŒ Failed to join group call - attempting recovery');
          setTimeout(() => peerConnection.restartIce(), 2000);
        }
      };

      peerConnection.oniceconnectionstatechange = () => {
        console.log('[CallContext] Join group call ICE state:', peerConnection.iceConnectionState);
      };

      // Add local stream to peer connection
      localStream.getTracks().forEach(track => {
        peerConnection.addTrack(track, localStream);
        console.log('[CallContext] Added local track for joining group call:', track.kind);
      });

      // Set up group call state
      const groupCallState: CallState = {
        callId,
        callType,
        status: 'connected' as const,
        isIncoming: false,
        localStream,
        remoteStreams: new Map(),
        peerConnection,
        audioEnabled: true,
        videoEnabled: callType === 'video',
        isMuted: false,
        startTime: new Date(),
        isGroupCall: true,
        groupId,
        groupName,
        participants: []
      };

      // Set active call state immediately for immediate participant sync
      setActiveCall(groupCallState);
      console.log('[CallContext] Active call state set for group:', groupId, 'callId:', callId);
      
      // This part is now handled by the enhanced processing below, removing duplicate

      // Send join message to server
      setTimeout(() => {
        const joinMessage = {
          type: 'join_group_call',
          payload: {
            callId,
            groupId,
            fromUserId: user.id
          }
        };
        
        ws.send(JSON.stringify(joinMessage));
        console.log('[CallContext] Sent join group call message for recipient:', joinMessage);
      }, 400); // Delay to ensure call state is set

      console.log('[CallContext] Joined group call successfully');
      console.log('[CallContext] Active call state after joining:', groupCallState);

      // Process any pending participant updates that arrived before activeCall was created
      console.log('[CallContext] ðŸ“Š Processing pending participant updates after join...');
      
      // ðŸ”¥ CRITICAL FIX: Process pending participant updates immediately
      const storedUpdate = localStorage.getItem('pendingParticipantUpdate');
      if (storedUpdate) {
        try {
          const pendingUpdate = JSON.parse(storedUpdate);
          console.log('[CallContext] ðŸ”„ Processing stored pending participant update:', pendingUpdate);
          
          // Force process the pending update now that we have activeCall
          setTimeout(() => {
            console.log('[CallContext] ðŸŽ¯ Force processing pending participant update with activeCall context');
            // Ensure the pending update has the correct structure
            if (pendingUpdate && pendingUpdate.payload) {
              handleGroupCallParticipantsUpdate(pendingUpdate);
            } else if (pendingUpdate && pendingUpdate.callId) {
              // Handle old format without wrapper
              handleGroupCallParticipantsUpdate({ payload: pendingUpdate });
            }
            localStorage.removeItem('pendingParticipantUpdate');
          }, 100);
        } catch (error) {
          console.error('[CallContext] Error processing pending participant update:', error);
          localStorage.removeItem('pendingParticipantUpdate');
        }
      }
      
      // ðŸ”¥ ENHANCED: Request fresh participant data from server as backup
      setTimeout(() => {
        console.log('[CallContext] ðŸ”„ Requesting fresh participant data as backup...');
        const requestMessage = {
          type: 'request_group_participants',
          payload: {
            callId,
            groupId,
            requestingUserId: user.id,
            forceRefresh: true
          }
        };
        
        if (ws && ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify(requestMessage));
          console.log('[CallContext] ðŸ“Š Sent backup participant request:', requestMessage);
        }
      }, 200);
      
      // ðŸ”¥ ENHANCED: Request fresh participant data from server as backup
      setTimeout(() => {
        console.log('[CallContext] ðŸ”„ Requesting fresh participant data as backup...');
        const requestMessage = {
          type: 'request_group_participants',
          payload: {
            callId,
            groupId,
            requestingUserId: user.id
          }
        };
        
        if (ws && ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify(requestMessage));
          console.log('[CallContext] ðŸ“Š Sent fresh participant request:', requestMessage);
        }
      }, 500);
      
      setTimeout(() => {
        // Force request participant update after joining
        const requestParticipantsMessage = {
          type: 'request_group_participants',
          payload: {
            callId,
            groupId,
            requestingUserId: user.id
          }
        };
        
        if (ws && ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify(requestParticipantsMessage));
          console.log('[CallContext] ðŸ“Š Requested updated participant list:', requestParticipantsMessage);
        }
      }, 500); // Wait for join to complete
      
      // Also immediately trigger WebRTC setup for current participants
      setTimeout(async () => {
        try {
          // Get updated participant names from server
          const response = await fetch('/api/all-users');
          const allUsers = await response.json();
          
          // Create user map for name lookups
          const userMap = new Map();
          allUsers.forEach((user: any) => {
            userMap.set(user.id, user.callsign || user.fullName || `User ${user.id}`);
          });
          
          // Dispatch event to GroupVideoCall to start WebRTC connections
          window.dispatchEvent(new CustomEvent('force-webrtc-init', {
            detail: {
              callId: groupCallState.callId,
              groupId: groupCallState.groupId,
              currentUserId: user.id,
              userMap
            }
          }));
          
          console.log('[CallContext] ðŸš€ Triggered WebRTC initialization for group call');
        } catch (error) {
          console.error('[CallContext] Error triggering WebRTC init:', error);
        }
      }, 1000);

      // Store the call state in localStorage to persist through navigation
      localStorage.setItem('activeGroupCall', JSON.stringify({
        callId: groupCallState.callId,
        groupId: groupCallState.groupId,
        groupName: groupCallState.groupName,
        callType: groupCallState.callType
      }));

      // Navigate to appropriate group call interface based on call type
      setTimeout(() => {
        if (callType === 'video') {
          setLocation('/group-video-call');
        } else {
          setLocation('/group-call');
        }
      }, 200);

    } catch (error: any) {
      console.error('[CallContext] Error joining group call:', error);
      // Only show alert for critical errors, not connection issues
      if (error.name === 'NotAllowedError' || error.name === 'NotFoundError' || error.name === 'NotSupportedError') {
        alert(getMobileErrorMessage(error).replace('Gagal memulai panggilan', 'Gagal bergabung ke panggilan grup'));
      } else {
        console.log('[CallContext] Non-critical error, continuing with call setup');
      }
    }
  };

  // Monitor state changes with useEffect
  useEffect(() => {
    console.log('[CallContext] ðŸ”¥ INCOMING CALL STATE CHANGED:', incomingCall);
    if (incomingCall) {
      console.log('[CallContext] ðŸ”¥ INCOMING CALL DETAILS:', {
        callId: incomingCall.callId,
        groupName: incomingCall.groupName,
        callType: incomingCall.callType,
        isGroupCall: incomingCall.isGroupCall
      });
    }
  }, [incomingCall]);

  // Debug logging for Context Provider value
  console.log('[CallContext] ðŸ”¥ PROVIDER VALUE UPDATE:', {
    incomingCall: incomingCall,
    activeCall: activeCall,
    hasIncomingCall: !!incomingCall,
    incomingCallId: incomingCall?.callId
  });

  return (
    <CallContext.Provider value={{
      activeCall,
      incomingCall,
      remoteAudioStream,
      ws,
      startCall,
      startGroupCall,
      acceptCall,
      rejectCall,
      hangupCall,
      toggleCallAudio,
      toggleCallVideo,
      toggleMute,
      switchCallCamera,
    }}>
      {children}
    </CallContext.Provider>
  );
}

export { CallContext };